1. FNN ( feed forward Neural Network)
Definition:
No.
Date
feed forward Neural Networks are
artificial Neural Network
which nodes do not form loops. This type of neural
net works is also known as a multi-layer nestal newral
network as all info is only passed forward.
in
During data flow, input nodes receivedate, which travel
though hidden layers, and exit output nodes.
No
links exist in the network that could get used to by
sending info- back from the output nod
Working principle of FNN
inputs
X, W
X20 W2
x3w3
Xy Wy
15. W5
y coos!
Neelgagan
No.
Date
No.
Date
when the fied forward neural network gets simplified
it can appear as a single layer perception.
"This model multiplics inputs with weights. as they enter
the layer. Afterward, the weighted off iff values get
added together to get the sum.
B
•As long as the sum
of the valves rises above a certain
threshold, set a cus, the off value is usually I,
while it falls below the threshold, it is usually-
•As a feed nemal network model, the single-layer perception
often gets used for classification. Through training
adjust their weights based on a
property
I called the [delta ru] which helps them compay
their outputs with the intended values).
newal networks can
Delta rule:
The della rule, also known as the Widrow - Hoff learning
rule
D
the Least Mean Squares (LMS) rule, is an algorition
used for training artificial neural networks. It is specially
designed for training singe -layer neural networks
(abo called perception)
and aims to minimize the
the actual o/p and the desired I
erros between
The delta rule calculates the
ved off.
viros by comparing
the actual off of the perceptron to the desired off
Neelgagan
[c=d-y
Layers of feed forward Neural Networks
1. Input Layer
The newsons of this layes receive input and pass it on
to the other layers of the network. Feature Drattribute
of neurons
numbers in the dataset must match the no..
in the i/player
2. Output Layer:
According to the type of model getting built, this layer
represents the forecasted feature
3. Hidden Layer:
I/p and off layers get separated by hidden Layers.
Depending on the type of model, there may be seved
hidden layers
4. Neuron Weights:
Neurons
get
connected by a weight, which measures their
strength or magnitud. Sime has to lineas regression
coefficients, imput wrights com abo get compared.
normally 5) w 0 and I, with a value b/w
Weight is normally.
Neelgagan
Dand
No.
Date
No.
Date
Neurons
Artificial neurons get used in FNN, which late gut
adapted from biological muurons. A neural network consists
of artificial neurone
Neusons
function
in two ways
first, they create weighted input sums, and
second, they activate the sums to make them normal.
6. Activation functions!
Neurons are responsible for making decisions in this area.
According to the activation function, the neuson determin
whether to make a
linear or nonlinear decision.
Thue maj a categoris
1.
sigmoid
input values b/w 0 and I get mapped to the off values
2. tamb
A value b/w -1 and / gets mapped, to the input values
3. ReLU IC Rectified Linear Unit)'
Only the palus on allowed to flow through this function..
Neelgagan
-ve values get mapped to 0.
Advantages of FNN oues athes DeepLearning architectures
FNN or MLPs have several advantages, especially in
urtain contexto, compared to this deep learning
architectur
1. Simplicity
2.
Q
straightforward architectur
lasics to undustand and implement
casies to train compared to complex architecture
Versatility
.
Genul purpose model
• No assumptions about input. to unlike CNN DIRNN
that assume spatial structors or temporal dependencie
respectively
3. Performs well on tabulas data
4. fever parameter twas avoids ourfitting
Limitation
• Not ideal for image data.
• Not suitable for sequential data.
Scalability ions
Neelgagan
fin my mind)
No.
Date
I 21 FNN stands for fest no forward feed Neural
net work then why it uses backp repagation?
If the term "feedforward Neural Network" refers to
to the architecture of the network and how data flows
through it during the forward pass, not the training
proun
Why the Names make sense?
FNN: This name
highlights the direction of data
flow during inference is prediction. The network
prouses impute and produces outputs in a forwar
manner without any cycles or feedback loops
Backpropagation: This refers to the metred and to train
the network by propagating veriors backward to adjist
the back propagation prous is not part of
the
weights
the normal operation of the network when making predicts.
-ons, it is specifically part of the training process.
No.
Date
2. Convolutional Neural Network:
CNN is a cotagory of ML mode, namely type of deep
learning algorithm well suited & to analyzing visual data.
a
In 2012, a significant breakthrough occurred when researcher
from the University of Toronto developed Alex Net, am Azmedu
I that significantly outperformed previous image recognition
algoritame. Alix Not, curated by Alex Krighewhy, won
2012 I may Net Contest with &s% accuracy, for surpassing
the Munner-f'o 74%. This sucus was driven by (NND)
• type of newal neturile that mimics human vinity
Pixes of ing fed
hidden laye
off
logy
image
• CNN have become fundamental in CV tasks such as
classification, object detection, and segmentation
Neelgagan
Neelgagan
No.
Date
What are convolutional Neural Network (CNN))
In DL, a Convolutional Neural Network (CNN/ Conv Net) is
Jass of dup neural netwerks, most commonly applied
•
a
to analyz
B
Visual
imagery
The CNN architecture uses a special technique called.
Convolution instead of relying solely on matrix multiplications
like traditional newal networks.
Convolutional networks uses a process called convolutionat,
which combines two functions to show how one changes
the shape of the other.
images
The role of the convolutional networks is to reduce the
into a form that is easier to prouess, without losing to features
that are critical for getting a good prediction.
How does CNN work?
• What is RGB image?
An RGB image is nothing but a matrix of pixel values having
three planes whereas a grayscale image is the same but it'
has a single plane
Neelgagan
2
0 76186/17/25/6
115 24 14 53
184370
813
08
3 Colous Channel
width units
Clixes
Height : 4 Units
(Pixel)
Grayscale images
0
D
0
Input Date
The
Neelgagan
O
Kernel
abour image shows what a
convolution is. We take a filter / kernel (3x)
matrix) and apply it to the imput
image to get the convolved feature. This
convolved feature is passed on to the next
Jayes..
Image
2
7
2
3/4
Conv olud
Featur
No.
Date
Convoluted
Feature
0*0=0
+0=0
1+0=0
4.
In the case
No.
Date
No.
of RGB color channel, Lab.
0 1671 167/119
0
D
0
0
0
00
0
0
0
D
000
0
0
156 155 156-158
0158
114 157 159
0 161 165 168 170
0161 ML 165 165
0760/161/164/12
019
151 155158
/70
0
186 188 187/152
0
141
158 163
0155 155 166 167
0/114/12/117/167
146 147 153
14143 143 148
Imput channel #1(Red)
162 166
0/10
0156 162159 118
0115113
Input channel #2 (Green) Input channel #3
(Blue)
0
0
1
-1
-1
0
P
OL
0
Kernel Channel
#1
Kernel Chamel
#2
-25 461
466/475
29 787 798 812
111-1
Kernel Channel
#3
bias
Date
•The number of parameters in a CNN layes depends on the Dize
of the receptive fields( filter kernels) and the number of filters
local
•fach neuron in a CNN layes receive impurts from.
region of the previous layes, knownes no receptive field. The
receptive fields move over the input, calculating
calculating dot product
beating a convolved feature
as the suffut.
and
map
trise Usually, this map then goes through a kelu activation
function.
modern ones like ResNet employ this fundamental principle
Classic CNN architectures like LeNet and more
CNN
are
composed of multiple layers of artificial nuuson
Artificial Nurons in CNN
• Artificial newsons, a rough imitations of this biological
counterparts, are mathematical functions that calculate the
weighted sum of multiple infacts and outfits mactivation
Value
When we input an image in Convict, each layer generates
several functions that are passed on to the next layer for
feature extraction.
Neelgagan
Neelgagan
No.
Date
No.
Date
Feature Extraction in (NN.
• The first layer usually extracts basic features such as
horizontal or diagonal e edges. This off is passed
on to the next layer which detects more complex features
edges.
suchos corners or combinational
As we move
deeper into the neturit, it can identify
wen more complex features such as
objects, fous, et
ConvNets are feed-forward network that process the imput
dats in a single pass.
• Based on the activation.
scores
map of the final convolution layer
In classification layer outputs a set of confidence
(values b/w 0 and I) that opecify how likely the image is to
belong to a "class"
Gradient descent is commonly used as the optimization
Algorithm dow during training to adjust the weights of the
imput layer and subsequent layers.
What is a pooling layer?
Similar to the Convolutional tayer, the pooling layer is responsible
for reducing the spatial size of the convolved feature. This is
to decrease the computational power required to process the
data by reducing the dimensions.
There are two types of pooling: average
D
average pooling
@max pooling.
Max poli
We find maximum value of a pixel from a portion of the image
covered by the kernel.
• It also performs
as a Nous suppresant, discards the noisy
activations altogether and also performs de-noising along to with
dimensionality reduction
①Average pooling:
• returns the average of all the values from the portion of the
image covered by the Kernel.
Note: Max Pooling performs a lot better than average pooling.
20 30
Max pooling
Nealgagan
Neelgagan
12
20 30 0
812
2
0
IL
31
34
70 37 4
13 8
112 100 25/12,
average booking
79
20
Fully connects
Neal Netun
4-77
with lo
P
Flattened
2 channel
nchang
(8x8x12) (7xuenes
2
Architecture of convolution Neural Network. (CNN):
convolution
Conv
(5x5) hernet
valid padding
Max-poolin
(212)
Kernel
valid, paniy
Convolution
Conv-z
(x5) k
New Connected
ReLU actuation
Pc-3
Fulls
Neelgagan
(28×28×1)
Channel
(24x24x1
Channel
(12x1221)
14
Neelgagan
No.
Date
No.
Date
Components of CNN:
① Convolutional Layer:
It tries to learn the feature representation of the inputs,
whither it be the images of isto vo dogs or digits.
- Cor computing the different feature maps, it is composed of
several kernels/ matrix which are weed.
• So, a filter / larmel of (non) matris depends on the type of
problem we are solving, and then it is applied to the iff
data (or image) to get the convolutions feature.
• This convolution features is then passed on to the next layer
after adding bias and applying any suitable actuation function.
Pooling Layer
• The porting layer is placed b/w the convolutional layers.
• used for achieving shift invariance which is achieved by decreasing
the resolution of the feature maps.
reducing the no of connections b/w convolutional layers,
the computational burden out on the processing units
①Fully Connected Layer
lowes
No.
Date
No.
Key components of CNN.
1. Convolutional Layer
multiplications
filters / Keend: Small matrices (eg, 3x3 or 5X5) that
slide over the input image, performing element wiss
and summing up the results to produce a single value
This process extracts features from the imput data
• Stride: The no of pixel by which the filter
4/pimage.
"Astrid of 1 means
while stride of 2
moves across the
th
filte
moves one
pixel at a time
means the it mores 2 pixels at a time.
5
mean no
Date
• Padding Adding extra pixels around the words of the app
image to contred the spatial dimensions of the ofp.
"" Valid " padding
。 padding, whil "same" padding means
padding the input so that the off size matches the imput
size.
2 Actuation function
Poding Laye
4. Fully Connected Layer
• After surial convolutional and fooling layer, the high-level
reasoning in the neural network is done via fully connected
Layers
these layers
networks, where each neuron is connected to
previous layer.
J.
are similar to those in traditional neural
every
neuson in the
"The outfout of the last convolutional or pooling layer is flattened
inte a ID vector and fed intoons or mor fully connected laye
5. Off layer
Neelgayan
Neelgagan
All CNN Architectures :
t. Le Net-5 (1998)
2. AlNet (2012)
3 nha Net (2014) VGG 16
4.
Google Net (Inception) (2014)
5. ResNet (2015)
6. Dense Net (2014)
VGG17
1. Mobil Net (2017)
8.
Shuffle Net (2017)
9.
Squeeze Net (2018)
10. Efficien Net (2019)
11. Xuption (2016)
12. Na NASNet (2018)
13. Reg Net C 2020)
14.
ConvNexTC2022)
No.
Date
No.
Date
*Trangler Learning
•
The reuse
of a pre-trained model on a new problem is known as
transfer burning in ML.
A machine uses the knowledge learned from a prior assignment
to increase prediction about a new task in transfer horning.
The knowledge of
already trained machine
Learning model
is transferred to a different but closely linked problem throughout
transfer barning.
• For example, if we trained a
whether
simple classifies to predict
an image contains a backpack, we would use the
model's training knowledge to identify other objects such as
sunglasses
Transfer learning involve using knowledge gained from on
task to enhance understanding in another. It automatically
shifts weights from a network that performed task B to a
not work performing task A ...
Steps in transfer learning
1. Choose a pre-trained model
.
4.
Sehet a model that has been pre-trained on large and divers
dataset (e.g. imagiNet)..
Common pre-trained model include VGC, ResNet, Inuption
and Efficient Net.
Neelgagan
Neelgagan
2. Remove the Output Layes:
No.
Date
• The final layer of the pre-trained model is typically
specific to the original task (e.g. classifying 1000 ty
ImogeNet classes)
.
Remove this layes the replace it with a new.
tailors to the new task
3. Freeze the Pre-trained layers:
"
them
layy (1)
Optionally, freeze the wrights of the earlier layers to prevent
from being updated during & training on the new task.
Freezing layers helps retain the useful features learned from
the original dataant
4. Add
4
hew
Layers
Add new layers to the model that are
opecific to the new
task (e.g. a new fully connected layer with the appropriate no of
outfut units for the new classification problem).
5. Train the new model:
•Train the new
layers on the target dataset while keeping th
pre-trained layers frozene
optionally, fine-tune some or all of the free-trained layers with
a lower learning rate to adapt them to the new task.
1. fine-tuning:
No.
Date
| Meaming! It refers to the process of taking a pre-trained
model and making omalladjustments to sto parameters
by continuing the training process on a new, often smally.
.
datast
• Unfreeze some of the earlies layers and continue training
with a smally barning rate to fine-tune the entire model.
This step cam help improu performanu by allowing the
pre-trained features to adapt slightly to the new task.
Advantages:
• Reduced training time
Less data required.
faster convergenc
D
Improved performanc
High accuracy
cost effective
Overcoming limited data.
Enhanus dato
augmentation
Leveraging state - of art models.
increment learning
•Cross-domain application.
Neelgagan
Neelgagan
Image Ne
Limitations
Lo competit
Domain Mismatch.
Overfitting
.
Model Size
:
No.
Date
Pre-trained models are
usually large, making
them computationally intensive and difficult to deploy
On bousu-constrained device
No.
Date
fooling
Aftiste 2 fully connected layers
"At last, a softmax classifies which classifies the images inte
respective class.
.
Dependency
en
2.1. LeNet-
•
4
by-trained modb.
LeNet-S is one of the earliest pre-trained models proposed by
Yann Le cum and other in 1998, They used this architectur
for recognizing the hand written and machine-printed
characters.
The main reason behind the popularity of this model was
simple and straight forward architecture! #
its
It is a multi-layer convolution neural network for image
classification.
Architecture of the model:
The netwal has 5 layers with learnable parameters and henc
named Lenet -S-
3 sets of convolution layers. with a combination of average
Nepagan
Input
32.X32X1
The input of to this model is a 32x32 grayscale image him
the no. of channels is 1.
Convolution
Op shape
((32-5+1)x(32-5+1)
x6)
Input: 32x32x1
Feature Map
28x28x4
2
(28x28x6)
We then apply the first convolution operation with the filter size 5x5
Neelgagan
No.
Date
and we have bouch filters. As a result, we get a feature map
of size (28x2886). Here the number of channels is equal to
the no of filters applied.
Convolution
(5x5)
subsampling
Z/P
(32x32x1)
Feature Maf
Feature Ma
(14X14X6)
(28x28x1)
No.
Date
Next, we have a convolution layer with 16 filters of Dizessi.
Again the feature map changed it is 10x10x16. The output siye
to calculated in a similar mannes After this
we again
applied
am
average pooling or subsampling layer, which again
reduce the size of the feature map by half ive. 5x5x16.
Convolutio
5x7
Subpply
Convolution
subsampling
Conv
lation
(5x)
J
After the first pooling operation, we apply the average pooling and
the size of the featur map is reduced by half. Note that,
the no. of channels is intact.
Convolution
CY
subsampling
Convolution
(5XV)
32x32x1
Feature Ma
(28x2946)
Feature Map
(14X14X6)
Feature Map
(10x10x167
Featur
Ma
featur
Ma
32×32×1)
28x28x6)
M
Featu
Featu
MY
120
(14X14X6) (10X18X16) (516)
+ Then we have a final convolution layer of size 5x5 with 120
filters.
Leaving the feature map size 1X1X120
result is 120 values.
with
After which flatten
After these convolution layers, we have a fully connected layer
righty-four (84) nemons. At last we have an off
layer with 10 neurons sing the data have 10 classes.
On next page, here is the final architecture of the Lenet -5 model.
Neelgagan
Neelyagan
LeNet Architecture
Convolution
Convolution
Subsampling
Subsamplify
convolution
Inpul
32x32x)
Featur
tow
9x84x67
Fully
lay
20
20
Map
14414x1
Featu
Featu
fou
Feat
dow
10x10x45x5x16
Neelgegan
No.
Date
Architecture Details
Layer
# filters/ filter
Stride
neurons
Size
Siyd
Feature Map
Actuation
Funchen
32x32X1
Imput
Conv
6
5x5
1
28x27x6
tamh
Aug. pooling
2x 2
2
14X14X6
Cem 2
16
5*5
1
10x10 XIL
tannh
Aug. pooliny 2
Conv 3
2x2
2
SX5XIL
120
5x5
L
120
tamh
24
tanh
10
softmax
Fully connected)
Fully connected 2.
Advantages
Simplicity, straight forward
Computational efficiency
• effective for small datasets
Limitations
:
• Limited depth
• Small filter size
Lack of modern techniques.
•Scalability issues
• performance.
No.
Date
Calculation
n+ap-b+1
stridi
2.2 AlexNet Architectur
f-filtersizs
budding D
No.
Date
• Alex Net won the Image Net large-scale vional recognition challeng
in 2012.
• The model was profed in the research paper by Alex Krizhevsky
3
4
4
and his colleagues.
In this model the depth of the network was increased in comparison
to Lenet-5
Alex het had eight layers with learnable parameters.
model consists of five layers with a combination of max
posting followed by 3 fully connected layere and they use
Relu actuation in each of these layers except the ofplayer.
They found out that using the relu as amactivation function
accelerated the speed of the training proux by almost six times.
Also used the dropout layers, that prevented this mode
from overfitting.
Dataset used Imaginct dataset
Alix Net architectus:
4 contains almost 14 million images
across thousand lasse
Sinu AlexNet is a deep architecture, the authors introdund
padding to prevent the size of the feature maps from redeving
dashially
input to the model:
Layer #filters / Filter
Imput
neuson
-
No.
Date
O/Pz Input-filter size + T
Dide
227822783
Output(Inputfilterical
Stride
Stride Padding Size of
Activation
Size
Learn Mop
function
227x227x3
Convl
96
Y
55x55x96
ReLU
MaxPool
-
3x3
2
27x27x91
Conv2
256
5x5
2
27x27x256
ReLU
Max Poul
3X3
2
13X13X256
Conv3
387
3x3
13X13X384
ReLU
Convy
394
3×3
13X13X384
ReLU
Convs
MaxPods
256
3X3
13X13X257
ReLU
383
2
6x6x256
6x6x256
4096
ReLU
4096
4096
ReLU
1000
Dropout rate=0.5
Fully Connects
Propout Mateor
FC 2
fc 3
SoftMove
•If we look at the architecture till now, the no. of filters is
increasing as we are going duper. Henu it is extracting.
we move deeper into the architecture
mou
features.
as
Neelgagan
No.
.
Date
•Also the filter size is reducing, which means the initial
filter was large and as we go ahead the filles size
is decreasing resulting in a decrease in the feature map snake.
Advantage
•
'
4
4
.
⋅
Depth and complexity
ReLU activation function
Dropout for Regularization (To combat overfitting)
Date Augmentation cam be employed
Local Respons. Normalization (LRN): It used LRN to
normalize the output of each meuson across the feature map
which helped in reducing overfitting and improved the
model's performanc
GPU utilization
Limitations
• High computational cost Clarge no. If parameters lead to memory was)
Limited depth (relatively shallow compared to modern architectures
Large filter size
4
3
• LRN : LRN is no
longer commonly used in modern architecture
because it das not provide significant benefits and can be
Computationally expensive. Batch normalization is typically
preferred for its regularization and performance benefits.
Lack of Modulas Design
Manual Hyperparameters
2.3 VG Net
No.
Date
It is a typically deep convolutional Neural Network (CNN)/design with
numerous layers.
vaa stands for Visual Geometry Group.
The term "deep defines the no. of layers, with VGG-16 Or
VGG - 19 having 16 as 19 convolutional layers respectively.
• Innovative objective identification models are built using the
VaG architecture.
VG Net
Layer1
Layer.
Conv
Conv
Layes
Conv
Conv 17
Layer
Conv
Tony
Layes Por
[[FC]
PET
Layes 6
FC
Lay Soft Max
Neelgegan
Neelgagan
No.
Date
No.
Date
.
•
VGG16
The convolutional neural network model called the VGG model, a
VGGNet, that supports 16 layers is also known as BVG616.
It was
developed by A. Zisserman and K. Simonyan from
The University of Oxford.
"It significantly outperforms Alex Net by substituting
several 3x3 hermal-sized filters for the huge kunel-sized
filters.
The VaG Net 16 has 16 layers and can classify photos into
100 diffrent object categories,
• The model abo accepts images with a resolution
2246 229.
•
VGG19
The VGG model Cabo known as VGG Net 19) has the
same basic idea as the VGG 16 model, with the exception
that it supporto 19 layers.
The number '16" and "9", refer to the model's 'wright layers
(convolution layers).
In comparison to VGG 16, VGG 19 contains three extra
convolutional layers.
752
12
Neelyngan
224x224 Input
224224 3x3 Convt ReLU 64
3x3 Ceny ReLUBY
(2x2 Max-Pool-64)
3x3 Conv RLV 128
2x2 Maxfool. 128
112X112 3x3 Conv +Real 128
3x3 ContReLU 256]
3x3 Conv ReLUS
56x56 3x3 Conv +ReLU 256
2x2 Max-Pool 256)
3K3 ConveReLU-512]
3x3 Conv+ ReLU SIL
23x18 3x3 conv + ReLU 512
Pool 512
14x14 3x3 ReLU 512
3x3 Conv +ReLUS12
Max-Pool 512
7x7 FC 4096
IFC 4096
FCO/P1000
Size:224
Size: 112
| Imput
3x3 Conul, 64
3x3 Conv267
Pod/2
3x3 Con 3, 128,
3 x3 cenu 7, 127
3x3
Size: 56
Size: 28
Size:14
Size: 7
Pool/2
Conv 5 256
[3x] Conv
3x3
5x Convo 254]
13x3 Convt L
(3x) Cemu 12
343 Con 12/512
3x3 Conv
Pool/2
3x3 Conu 135/L
33
3x3 con 14 512
13x3 Lou 15,512
Pod
4096
fc 4091
16.1000
Neelgagan
No.
Date
No.
Date
Vicent
Comparison b/w VGG-16 and vac-19.
Featur
Total Layers
fully connected
Max pooling layer
rac-16
16 weight layers
33
S
2x 2
VaG-19
19 weight layer
+63
2x2
Layes
5
Convolution filter
3x3
3x3
size
Poding layersize
No-parameters
~133 million
~144 million
(2+2+414+4)
Input image size
224x2243 (RGB)
Actuation n
Flayes
Output laye
224x224XRB
13
extraction
16
Depth Configuration (2+2+3+3+3
Convolution laye
Typical Uss loss
Relvafter each Convand PC ReLU after each conv and
1000-way Softmar
1000-way soft max
Image Classification feature Image Vassification, feature extraction
High acumaly, suitable
Higher than V GG-16 due to
more parameter.
Relatively high due to Higher than NGG-16 due to
depth and parameter more parameters.
Very T
Performany
for transfer learning
Training time
Computational Cost
T
Memory Usage
1-
Model Siz
Deployment
Large
Suitable for or cloud
deployment
Suitable
for h
PU OA
Neelgagan
Very T
Very Large
cloud
deployment
.
62 Google Net
Inception Modul
• The inception module is a building block for convolutional Neurd
Networks (CNN) introduced by Google researchers in their
seminal paper "Going Deepes with convolutions" in 2014.
•This architecture, also known as Google Net, represented a
The inception module is designed to allow a CNN to benefit from
multi-level feature extraction by implementing
4
various sizes in the same layer of the network.
Key features:
Multi-level feature extraction
Dimensionality reduction
• Pooling'
.
Evolution
features of
The inuption module has evolved through several ituation,
leading to improved versions such as Inception 12, Inceptions
and Inuption -14.
These versions have introduced various optimizations,
including factorization of convolutions, expansion of the filter
bank outputs, and the use of residual connections.
One notable variant is the Incipition -ResNet hybrid,
combines the inception architecture with residual connections
which
Neelgagan
No.
Date
No.
Date
from ResNet.
Google Net
• Google Net, released in 2014, oct a new benchmade in object
classification and detection through its innovative approach
(achieving a top-5 vera rate of 1.1%, nearly half the
•
veros rate of the previous year's winner. ZF Net with 11.7%)
Googl Net's deep learning model was defer than all the previous
mocks released with 22 layers in total
• Increasing the depth of the Machine Learning model is entupir
as duper models tend to have more learning capacity and
as a result
this increases the performance of a model.
Only posible if we solve vanishing gradient prostem.
9
The inception module, the key innovation introduced by a
team of Google researchers solved this problem. Instead of
deciding what filter size to use and when to perform a
pooling operation, they combined, multiple convolution
max
filters...
Neelquyun
Detailed Inception Module Explanation
The inception module is a key innovation introduced in the
inception (GoogleNet larchitecture. It allows the neturach to
efficiently captors and process info. at multiple scales by
applying several different types of convolutional and pooling
operations in parallel.
Structure of
an
Inception Module:
An inuption module typically consist of the following components-
"
IXI Convolution
• Reduus the no. of input channels (dimensionality reduction,
before applying more computationally expensive. Convolutions
(like & 3x3 and 5X;). This helps to reduce computational
cost
343 Convolution:
• Captures medium-sized features from the input feature map.
Before applying a 3XI convolution, a (X) convolution is often
used to reduce the number of input channels, which makes
the process more
efficient
3Sxi Convolutions
Captures larger features from the input feature map. Similar to
Neelgagan
No.
Date
.
3x3 convolution, a IXI convolution is applied beforehand to
reduce computational expense.
3x3 MaxPoding
"Helps in capturing spatial relationships and invarianc
max
pooling
a
I convolution is applied to the
After
pooled feature map to control the dimensionality
Concatenation:
The outputs of the above operations and are concatenated along
the depth (channel) dimension to form the final outpute
of the inception module. This results in a combination of
feature maps at different scales.
Nelgagan
Architecture
tecture of Google Net
No.
Date
type
patch o/poize depth #11 #3x3
piy/stride
Convolution 7x7/2
redu
reduce
#3×3 #5x5 # 5x5 pod ops.
pros
112X1124
34M
maxpool 3x3/2
56x56x14
0
Convolution 3x3/1
56x56x192
2
64
192
31OM
maxPool 3x3/2
28x28x/92
inuphora)
28x28x256
Inception (35)
28x28x420
722
64
96
128 16
32
32
122M
128
128
192
32
6
91
64
304
maxpool 3x3/2
14X14X430
0
inception (7a)
14X14X512
2
192
91
208
IL
24
(4
73 M
inciptionly by
14X14X512
2
160
112
224 24
64
164
88 M
inception res
14X1YKSIL
2
120
122
256
24
by
64
loot
inception
Md
112
144 29
32
6y
64
1191
Thuphin (re)
14X17X832
2 25 120 320
32
123
122
1701
marpool 3×3/2 7x7x832
0
inuptional
7X7X832
2
251
160
320 32
128
127
SUM
inception (5)
2
7x7x1024
384 192
384
42
128
21
aug pool 7x7/1/1X1X1024
D
dropout
XIX1024 O
(40%)
linear
IXIX/000
IM
tmax
D
IXIXION
Neelgagan
No.
Date
of Inception
• GoogleNet model. is particularly well-known for its use
modules, which serve as its building blocks by using parallel. convolutions
with various filter bije (vel, 3x), and 5x5) within a single
layer.
The outputs from these filters
are concatenated.
$
Moreover, the architecture is relatively dup with 22
layers, howevy
the model maintains computational efficiency despite the
increase in the no. of layer.
Key features of Google Net
• Inuption Modul
°
•
The X1 Convolution
Global Average Pooling
Puxisory Auxiliary. Classifies for training
Filter concatenahon
3x3 Conv
SKS Conv
1X1 Conv
T
Ix Conv
1X1 Conv
1x1Conv
3x3 Mar Pooling
•Input featur Map Size: 28x28.
•Input Channels (0) 192
• No. of filtus in 3x3 Conv (F):96
# (lobal Average Pooling
C
No.
Date
It is a CNN technique in the place of fully connectal layer
at the end part of the network. This method reduces the total
no. of parameters and minimizes overfitting
# Auxiliary Classifiers for Training
"These are intermediat, classifier found on the side of the network.
These are
only used during training and in the inference, these are
emitted
Auxiliary classifies help overcome the challenges of training very
Deep Neural Networks, and vanishing gradients (when the gradients
turn into extremely small values).
featur
• In Google Net architecture, there are two auxiliary classifiers in the
network. They are placed strategically, where the depth of the
extracted is sufficient to make a meaningful impact, but befor
the final prediction from the off classifice.
Neelgagan
Previous Layer
Neelgagan
No.
Date
No.
Date
Structure of each auxiliary classifier.
.
An
average poding layer with 5x5 window and stride 3.
• AIX) convolution for dimension reduction with 128 filters
.
Two
connected
layers
the first layer with 1024 unite,
fully
followed by a dreport layer and the final layer corresponding.
to the no. of classes in the task.
Apoftmax layer to o/p the prediction probabilities
+ There auxiliary classifiers help the gradient to flow and not
diminish too quickly, as it propagates back through the staf
dufer layer. This is what makes training a deep neural
network like Google Net possible.
→ Moreover, the auxiliary classifiers also help with model
regularization. Sinu each classifies contributes to the final
output, as a result, the network distributes its learning
different parts of the network.
acron
→ This distribution prevents the network from relying too heavily
on specific features or layers, which ruduus the chances of
ourfitting.
Neelgagan
Inception Family Varianto
Google Not (Inception V₁)
introduction of inuption modules, auxiliary Jassifics,
global average pooling
2014
V2
2. Inception v2
Batch normalization, factorized convolutions (eg, 5x5.
repland by two 3×3), improved training
peed
2015
3. Inception v3
"Further factorization (eg., (x7 and 7x1), asymmetric
convolutions, label smoothing
-
2015
4. Inception vs
Increased depth and
techniques
5. Inuption - ResNet V,
and complicity, advanced factorization
Combination of inception modules with residual connections
Enhamud version of Inception Respet VI with further improvement
Neelgagan
No.
Date
No.
Date
Vanishing Gradient Problem:
The
Vanishing
many
Gradient Problem is a common issue in the training
of dup neural networks, particularly those with
layers. It occurs when the gradients of the loss function with
respect to the network's parameters become very small
as they propagated backward through the network during training.
This makes it difficult for the network to learn and update
its parameters effectively.
#Causes:
1. Activation functions like sigmoid and tank can squash their input
Anto
avery
small range
2. Deep Networks: the multiplication of many small gradients
during backpropagation can lead to an exponentially small gradient."
3. Weight Initialization:
•
Poss weight initialization cam exacerbate the vanishing
gradient problem.
If weights are initialized with very small values, the
gradient
com
quickly become too small to be useful.
# Consequences:
very
vuy
alow because
Amal
.
1. Slow convergeny Training becomes
the updates to the network's parameters an
2. Poor Performance: The network may fail to barn verful
features, especially in the early layers.
#Solutions!
1. Activation functions
2. Weight Initialization
ReLU and it's varianto.
•He Initialization: For ReLU and it's varianto, He
initialization (random initialization) from a distribution
with zero mean and variance scaled by
units) helps maintain the gradient flow.
the mo
mo. input
• Xavier Initialization: For sigmoid and tamh activation
functions, Xavies initialization helps by scaling the weig
according to the no. of input and output units.
3. Batch normalization
4. Residual Connections
a
ResNet
· Highway Networks.
wrights
Neelgagan
Neelgagan
No.
Date
No.
2.5 ResNet Architecture
• ResNet stands for Residual Neural Network and is a type of
convolutional Neural Network (CNN).
C
It was
designed to tackle the issue of vanishing gradients
deep networks, which was a
deep neural networks.
majo
hindrance in
in
developing
The ResNet architect us enables the network to learn multiple
layers of features without getting stuck in local minima, a common
issue with
deep
Features of ResNet:
.
•Residual connection
networks.
as the
Identity Mapping ResNet uses identity mapping
residual function, which makes the training proass easier
by learning the residual mapping rather than the actual mapping.
Depth: ResNet enables the creation of very deep neural networks,
which can improve performance on image recognition
fash
Fewer parameters: ResNet achieves better results with fever
parameters, making it computationally more efficient
State-of-the-art Results: ResNet has achieved state-of-
the art results on various image recognition tasks and
has become a widely
used benchmark for image recognition
tasks
How ResNet works?
au
otached
Date
to forma ResNet.
• Many residual blocks.
together
We have "shipped connections" which are
ResNet
9
the
major part of
The idea is to connect the input of a layes directly to the
output of a layer after skipping a few connections.
Weight Layer
f(x)
relu
Identity
(Weight Layer)
{(x)+ x Plus
to
directly
Here, x is the input of the layer which we are
using
to connect to a layer after shipping identity connections
at the identity connections and if we think the output
from identification connection to be f(x). Then we can
say that output will be f(x)+x.
Neelgagan
Neelgagan
•
4
A
No.
Date
and f(x) may vary
• One problem that may happen is regarding the dimensions.
Sometimes the dimensions of x
and this needs to be solved.
4
Two approaches
can be
followed in such situations:
1. involves padding the input x with weights such as it
of the
coming out.
how brought equal to that
2. include using a convolutional layer from x to addition
to flx)
value
The skip connections in ResNet solus the problem of
vanishing gradient in deep neural networks by allowing this
alternate shortest path for the gradient to flow through
The complete idea is to make F(x)=0. So that at the
end we have Y = X as result. This means that the
which
value. coming out from the activation function of the
identity blocks is the same as the input from
shipped the connections.
ResNetso
we
ResNetso is CNN architecture that belongs to the Res Net
(Residual Network family, a series of models designed to
associated with training deep neved
addres the
networks.
Challenges
• Developed by researchers at Microsoft
Neelgagan
No.
Date
•ResNet architecture comes in various depths, such as ResNet-12,
ResNet -32, and so forth with ResNet so being a mid-sized variant.
It w
was released in 2015
ResNet and Residual Blocks
• The primary problem ResNet solved was the degradation problem
in deep neural networks. As networks become deeper, their accuracy
saturates and degrades rapidly.
• This degradation is not caused by overfitting, but rather the
difficulty of optimizing the training process.
20
training error (y)
a
10
m
2
ity.
3
16-layer
Y
20. lages
test essor (1)
20
ires.
56-laye
20-lagu
The deepes neturer has higher training error and this test ersor
ResNet solved this problem using Residual Blocks that allow for the
direct flow of information through the ship connections.
Neelgagan
No.
Date
• The residual block used in Res Net 50 is called the Bottleneck
4
Residual Block.
26-d
No.
.
Date
a filter size of 3x3
The second convolutional layer might use a
to extract spatial features from the data
The third convolutional layer again uses a filter size of 1x1 to
restore the original no of channel before the off is added to
the short cut connection.
3. ship Connection
3x3, 4
1x1256
rulu
Breakdown of the architecture or the residual block.
1. Relly Activation
2. Bottleneck convolution Layers:
The block consists of three convolutional layers with batch
normalization and ReLU activation after each :
B
filter size
• The first convolutional layes likely mais a
of 1x1 and reduces the no. of channels in the input data.
The dimensionality reduction helps to compress the data and
improve computational efficiency without sacrificing too
much information
Stacking
the Blocks Building ResNet-50.
.
ResNet-50 incorporates so bottleneck residual blocks,
arranged in a stack manner. The early layers of
the network feature conventional convolutional and pooling
layers to preprouss the image before it undergoes further
processing by residual blocks.
ultimately, fully connected layers positioned at the pinnack
of the structure utilize the refined data to categorize the image
with prevision.
Nenigayan
Neelgagan
2-6
DenseNet
No.
Date
its destination.
No.
Date
- Demar Net is densely connected -convolutional networks.
It is very similar to a ResNet with some fundamental
differences. ResNet is using
an additive method that
means they take a previous output
9am
input for a
future layer, and in Dense Net takes all previous.
output
below
223
imput for a future layer as shown in to
10000
No of connections in Dense Net =
LCLU
2
• In dense net, we have less no- of layers than the other.
model, so hen we com train more than 100.
Totonates modt of the model very easily by using
this technique
Input
Dena Block
fooling
Leather prizes
reduce feature
Dense Block 2
layers
4
Dende Net architecture was specially developed to improv
accuracy caused by vanishing gradient in hig input
neural networks due to the
long distance b/w input
and off layers and the info.. vanishes before reaching
Neelgagan
off a
Neelgagan
Peeling
Dense Blocks
a
Convolution
Cooling
feature map sizes match within each
block
•
.
No.
Date
Ao we go deeper into the network this becomes a kind
of unsustainable, if we go 2nd layer to 3rd layer so
3rd layer takes an impout not only 2nd layes but it
takes imput all previous layers.
Let's
will
layer
Day we have about 10 layers. Then the 10th
take us to input all the feature maps from the preceding
nine layers. Now if each of these layers, let's produce
122 features. maps and there is a feature map explosion,
to ourcom this problem we create a dense block here and
so each dense black contains a prespecified no. of layers
inoid them and the o/p from that particular dense block
to what is called a transition layer and this
is given
layer is like IX I convolution followed by Marpooling
to reduce the size of the feature maps.
• So the transition layes allows for max pooling, which typically
leads to a reduction in the size of the feature maps.
B. The fig onth previous page shows the two blocks first on
is convolution layer and the second is the pooling layer and
Combinations of both asse transition layer.
Concepts in Dense Net
No.
Date
• Growth Rate: This determines the no. of feature maps off
into individual layers inside dense blocks.
• Dense Conectivity: By dense connectivity, we mean
that within a dense block each layer gets us imput
feature maps from the i/p feature maps from the previous
lages.
°
Composite functions: Sequence of operations :-
- batch normalization
· Transition layers
2.1 Mobile Net V2 :
•
1
ReLU
Convolution Layer
A light weight CNN architecture, Mobile Net V2, is specifically
designed for mobile and embedded vision applications.
The model's ability is to strike a good balance between
model size and accuracy, rendering
- Constrained devices.
it ideal
for
resousu
Neelgayan
Neelgagan
Why
128x128x3
prproaning
ReLU
3x3 Conv
4-32
Max pool
5=96
22
ReLU
3x3 Conv
No.
Date
42/290
411280
T
Resprousing
128x128
64x64
32x32
Mobile Net V2
use Mobile Net V2 for Image Classification?
4x4
①It's lightweight architecture allows for efficient deployment
on mobile and embedded devices with limited computational
resources.
It achieves competitive accuracy compared to large and
more computationally expensive modib.
model's smally size enables faster inferemetimes,
making it suitable for real-time applications.
Neelgagan
Mobile Net V2 Architecture
Depth wise separable Convolution
No.
Date
It is a technique used in Mobile Net V₂ to reduce the
Computational cost of convolutions. It separates the standard
convolution into separate operations: depth wis convolution
and pointwis convolution.
• Inverted Residuals
the model'ea
It is a key component of Mobile Net VL that helps improve
e accuracy. They introduce bottleneck structur
that expands the no. of channel before applying depth wise
separable convolutions. This expansion allows the model to
capture more complex features and enhance its representat
-ion power.
Bottle neck Design
It reduces the computational cost by using IXI convolutions
to reduce the no if channel before applying deftruise
separable convolutions. helps maintain a good balance b/w
model size and accuracy-
· Linear Bottlenecks:
the
It is used to address the issue of info.com during
bottleneck frouss By using lineas activations.instead of
non-linear activations, the model preserves more information
Neelgagan
No.
Date
and improves it's ability to capture fine-grained details.
•Squeeze-and- Excitation (SE) Block:
They
are added to enhance its feature representation
capabilities. These block adaptively recalibrate the channel-
wise feature explosis responses, allowing the model
and suppres
on more informative features
to four.
les relevant ones.
2.8 Efficient Net:
• The technique of randomly scaling model requires
manual tuning and many & person hours, often resulting
in little or no noimprovement in performane
• The authors of Efficient Net proposed scaling up CNN modib
to obtain better accuracy and efficiency
ifficiency in a much mor
·
moral
way.
and
Introduced in 2019, it became a go-to architecture for many
tacks, including object recognition, image segmentation
even language proussing.
Ability: Computational efficiency and model performan
resolution) in a
By systematically scaling the model's dimensions (width, depth and
a principled manner, Efficient Net achieves
unprecedented levels of efficiency without compromising actively
Neelyayan
•
No.
Date
"
It is build upon the concept called compound scaling"
•The idea behind compound scaling is to ocal thre essential
dimensions of neural network: width, depth and resolution
1.Width:
• refers to the no. of channel in each layer of the newral
netwo
↑ width, model Com capture more complex patterns
and features, resulting in improved accuracy
I width, led to a mon lightweight model, suitable for
low-resouse environments.
2. Depths: Dapto
•
scaling pertains to the total no. of layers in the network.
• Deeper models can capture more intricate representations
of date, but they also demand more computational resources
shallower models
sacrifice accuracy.
3. Resolution:
are
computationally
efficient
but
may
Scaling involves adjusting the imput image's size.
• ↑ resolution images provide more detailed info.,
potentially leading to better performance. However, they
abo require mon memory and computational power.
Neelgagan
No.
Date
No
consume
fewer
fine-grained details.
• I resolution images,
lead to a
los in
How compound scaling works
resourus but may
•The prouss begins with a baseline model, which serves as
This baseline model is usually a
the starting point,
Masonably sized network:
a user-defined
• Then a compound wifficient is introduced as a
parametes that dictate how much to seal the dimensions of
the neural network. It is a single scalar value that uniformly
scales the width, depth and resolution of the
By adjusting the of vale. The overall. complexity and resource
requirements of the model can be controlled.
modd
• The arding factors for each dimension are derived from
the compound coefficient &.
Neelgagan
→ Width scaling! The width of the neural network is cated
proportionally by raising & to the power of a
T
specific exponent (typically denoted as α)
→ Depth scaling: Similarly, the depth of the network
is scaled by raising & to another exponent (denoted as pe
→ Resolution scaling: The resolution or input image size
is tocaled by multiplying the original valution by
°
Date
& raised to a different exponent (usually denoted as Y)
Optimal exponents need to be determined.
+ The values of these exponents are typically derived
through empirical grid ocanh us optimization process.
• Smaller & more lightweight and resouse-efficient
modib.
larges &
more powerful but computationally
intensive modeb.
Architecture
• uses Mobile invited Bottleneck CMB Cons) layers. which
are a combination of depth-wise separable convolutions
and inverted residual blocks.
2
use
SE.
T
Input Imag
Conv 3x3
MBConv1, 3x3] Jo Alak 1
MB Convb., 3x3
MB Lenu 6, 3x3
[MB Lon√6,5x57
MB convoy, 5x5
MBConv6 3x3
Mblonus, 3x3
MB Conu 6, 3x3
MPConv6,5x5
MB conv6 SX
MBConv
FDC, 575/
MD Convé, 5x5
MBConv6, 5x5
(MB Conv 6, 5x5
MB temu, 33
Fatature Map.
Block 2
Block 3
Hi Blocks
Bloch -6.
I Block
Neelgagan
No.
Date
IOCR
OCR stands for Optical Character Recognition
-
It is used to describe algorithms and techniques (both electronic
and mechanical) to convert images of text to machine - encoded
text.
These are the systems that I
7 Accept
generated)
a
am
input image (scanned, photographed, or computer-
→ Automatically
would
detect the text and "read" it as a human
→ Convert the text to a machine readable format so that it
cam be searched, indexed, and prouessed within the scope of
lg larger computer vidion systems.
com also be mechanical and cal physical
OCR systems!
→ Digital Pens
Tesseract OCR
Cloud API
+ Custom trained models
-
OCR is hard
+ There are so many nuances in how humans Communicate
via writing
+ Different writing styles, different fonts,
Neelgagan
No.
Date
No.
Date
J
Documents that are
noisy, dirty, distorted, etc.
We have all the problems of Natural Language broussing (NLP),
compounded with the fact that computer vision systems
obtain 100% accuracy
when reading a piece of
will never
text from an image.
- History of Ock!
T
Early Ock technologies were purely mechanical, dating back he
1914 when Emanuel Goldberg developed a machine that could
read characters and then converted them into standard telegraph
code
Goldberg continued his research into the 1920s and 1930s when
he developed a system that searched mirifilm (scaled-down
documents, typically films, newspapers, journals, etc) for
Characters and them OCed them.
+ In 1974, Ray Kurzweil and Kurzweil Computer Products, Inc.
Continued developing OCR systems, mainly focusing on creating
a "reeding machine forthe blind":
+ Kurzweil's work caught Xerox's attention, who wished to commercialize
the software further and develop OCR applications for document
understanding
T
Hewlett-Packard (HR) Labo started working on Texeral in the
19805
HP's work was then open-sourced in 2005, quickly becoming
the world's most popular OCR engine
Tesseract library plays important part in OCR
+ AD Deep Learning revolutionized the field of CV in the 20100,
OCR accuracy
was given a tremendous boost
from specialized architectures called Long Short Term Memory
CLSTM) Network
Now in the
20200
we're
acing how OCK has become increasingly
commercialized by tech giants such as Google, Microsoft and
Amazon, etc.
→ We exist in a fantastic time in the computer science
field
still take a
But the fact remains that these OCK engines
knowledgeable.computes vision practitiones to operate
- Applications of OCR =
"Automatic license/ number plate recognition (ALPOR/ANPR)
I Traffic Sign Recognition
Analyzing and dobry defeating CAPTCHAD on websites.
Neelgagan
No.
Date
+ Extracting information from business cards.
Automatically reading the machine-readable zone (MR2) and
other relevant parts of a passport
J
+ Parsing the routing number, account number, and curreny.
amount from a bank cheque
+ Understanding text in natural sumes such as
captured from the smartphone
- Orientation and Script Detection (OSD)
the photos
of och is the process of taking an input image and returning
the text in both human-readable and machine-readable
formats, then OSD is the process of analyzing the image for text
I meta-data, specifically the orientation and script /
ww
tingstyl
+ The text's orientation is the angles in degrees) of the text in an
input image
OSD to
+10 obtain highes OCK accuracy, we may need foot fly OCR.
determine the text orientation, it and
→ Script and Writing styl refers to a set of characters and symbols
used for written and typed communication
→ Most of us are familias with Latin Characters, which moke of
the Characters and symbols werd by many European and Western.
countries; however there are many other forms of writing styles
widely used, including Arabic, Hebrery Chinese, &c.
that are
No.
Date
Latin Characters are very different from Arabic,
which is, in turn, distinct from Kanji, d'oystem of Japanese
writing the using Chinese characters.
→ Any rules, heuristics, Orassumptions an OCR system.com moke
regarding a particular script is writing system will make the
engine
that much more accurate when applied to a given
-
ock
script
Therefore, we may use OSD information as a precursor
to improving OCR accuracy
The importants of Post - Prousing and Pre-Prousing
→ Many OCR engines (whether Texeract or cloud-based APID)
will be more accurate if we com
we can apply computer vision and
image processing techniques to clian of the image.
T
If we were to apply some basic image processing
operations such as thresholding, distance transforms and
morphological operations, un would end up with a lear image
When we pay the chaned up image through Tesseract,
we will (ideally) obtain more accurate results
Nealgrynn
Neelgagan
that they
assumewhen the fied forward neural network gets simplified
it can appear as a single layer perception.
"This model multiplics inputs with weights. as they enter
the layer. Afterward, the weighted off iff values get
added together to get the sum.
B
•As long as the sum
of the valves rises above a certain
threshold, set a cus, the off value is usually I,
while it falls below the threshold, it is usually-
•As a feed nemal network model, the single-layer perception
often gets used for classification. Through training
adjust their weights based on a
property
I called the [delta ru] which helps them compay
their outputs with the intended values).
newal networks can
Delta rule:
The della rule, also known as the Widrow - Hoff learning
rule
D
the Least Mean Squares (LMS) rule, is an algorition
used for training artificial neural networks. It is specially
designed for training singe -layer neural networks
(abo called perception)
and aims to minimize the
the actual o/p and the desired I
erros between
The delta rule calculates the
ved off.
viros by comparing
the actual off of the perceptron to the desired off
Neelgagan
[c=d-y
Layers of feed forward Neural Networks
1. Input Layer
The newsons of this layes receive input and pass it on
to the other layers of the network. Feature Drattribute
of neurons
numbers in the dataset must match the no..
in the i/player
2. Output Layer:
According to the type of model getting built, this layer
represents the forecasted feature
3. Hidden Layer:
I/p and off layers get separated by hidden Layers.
Depending on the type of model, there may be seved
hidden layers
4. Neuron Weights:
Neurons
get
connected by a weight, which measures their
strength or magnitud. Sime has to lineas regression
coefficients, imput wrights com abo get compared.
normally 5) w 0 and I, with a value b/w
Weight is normally.
Neelgagan
Dand
No.
Date
No.
Date
Neurons
Artificial neurons get used in FNN, which late gut
adapted from biological muurons. A neural network consists
of artificial neurone
Neusons
function
in two ways
first, they create weighted input sums, and
second, they activate the sums to make them normal.
6. Activation functions!
Neurons are responsible for making decisions in this area.
According to the activation function, the neuson determin
whether to make a
linear or nonlinear decision.
Thue maj a categoris
1.
sigmoid
input values b/w 0 and I get mapped to the off values
2. tamb
A value b/w -1 and / gets mapped, to the input values
3. ReLU IC Rectified Linear Unit)'
Only the palus on allowed to flow through this function..
Neelgagan
-ve values get mapped to 0.
Advantages of FNN oues athes DeepLearning architectures
FNN or MLPs have several advantages, especially in
urtain contexto, compared to this deep learning
architectur
1. Simplicity
2.
Q
straightforward architectur
lasics to undustand and implement
casies to train compared to complex architecture
Versatility
.
Genul purpose model
• No assumptions about input. to unlike CNN DIRNN
that assume spatial structors or temporal dependencie
respectively
3. Performs well on tabulas data
4. fever parameter twas avoids ourfitting
Limitation
• Not ideal for image data.
• Not suitable for sequential data.
Scalability ions
Neelgagan
fin my mind)
No.
Date
I 21 FNN stands for fest no forward feed Neural
net work then why it uses backp repagation?
If the term "feedforward Neural Network" refers to
to the architecture of the network and how data flows
through it during the forward pass, not the training
proun
Why the Names make sense?
FNN: This name
highlights the direction of data
flow during inference is prediction. The network
prouses impute and produces outputs in a forwar
manner without any cycles or feedback loops
Backpropagation: This refers to the metred and to train
the network by propagating veriors backward to adjist
the back propagation prous is not part of
the
weights
the normal operation of the network when making predicts.
-ons, it is specifically part of the training process.
No.
Date
2. Convolutional Neural Network:
CNN is a cotagory of ML mode, namely type of deep
learning algorithm well suited & to analyzing visual data.
a
In 2012, a significant breakthrough occurred when researcher
from the University of Toronto developed Alex Net, am Azmedu
I that significantly outperformed previous image recognition
algoritame. Alix Not, curated by Alex Krighewhy, won
2012 I may Net Contest with &s% accuracy, for surpassing
the Munner-f'o 74%. This sucus was driven by (NND)
• type of newal neturile that mimics human vinity
Pixes of ing fed
hidden laye
off
logy
image
• CNN have become fundamental in CV tasks such as
classification, object detection, and segmentation
Neelgagan
Neelgagan
No.
Date
What are convolutional Neural Network (CNN))
In DL, a Convolutional Neural Network (CNN/ Conv Net) is
Jass of dup neural netwerks, most commonly applied
•
a
to analyz
B
Visual
imagery
The CNN architecture uses a special technique called.
Convolution instead of relying solely on matrix multiplications
like traditional newal networks.
Convolutional networks uses a process called convolutionat,
which combines two functions to show how one changes
the shape of the other.
images
The role of the convolutional networks is to reduce the
into a form that is easier to prouess, without losing to features
that are critical for getting a good prediction.
How does CNN work?
• What is RGB image?
An RGB image is nothing but a matrix of pixel values having
three planes whereas a grayscale image is the same but it'
has a single plane
Neelgagan
2
0 76186/17/25/6
115 24 14 53
184370
813
08
3 Colous Channel
width units
Clixes
Height : 4 Units
(Pixel)
Grayscale images
0
D
0
Input Date
The
Neelgagan
O
Kernel
abour image shows what a
convolution is. We take a filter / kernel (3x)
matrix) and apply it to the imput
image to get the convolved feature. This
convolved feature is passed on to the next
Jayes..
Image
2
7
2
3/4
Conv olud
Featur
No.
Date
Convoluted
Feature
0*0=0
+0=0
1+0=0
4.
In the case
No.
Date
No.
of RGB color channel, Lab.
0 1671 167/119
0
D
0
0
0
00
0
0
0
D
000
0
0
156 155 156-158
0158
114 157 159
0 161 165 168 170
0161 ML 165 165
0760/161/164/12
019
151 155158
/70
0
186 188 187/152
0
141
158 163
0155 155 166 167
0/114/12/117/167
146 147 153
14143 143 148
Imput channel #1(Red)
162 166
0/10
0156 162159 118
0115113
Input channel #2 (Green) Input channel #3
(Blue)
0
0
1
-1
-1
0
P
OL
0
Kernel Channel
#1
Kernel Chamel
#2
-25 461
466/475
29 787 798 812
111-1
Kernel Channel
#3
bias
Date
•The number of parameters in a CNN layes depends on the Dize
of the receptive fields( filter kernels) and the number of filters
local
•fach neuron in a CNN layes receive impurts from.
region of the previous layes, knownes no receptive field. The
receptive fields move over the input, calculating
calculating dot product
beating a convolved feature
as the suffut.
and
map
trise Usually, this map then goes through a kelu activation
function.
modern ones like ResNet employ this fundamental principle
Classic CNN architectures like LeNet and more
CNN
are
composed of multiple layers of artificial nuuson
Artificial Nurons in CNN
• Artificial newsons, a rough imitations of this biological
counterparts, are mathematical functions that calculate the
weighted sum of multiple infacts and outfits mactivation
Value
When we input an image in Convict, each layer generates
several functions that are passed on to the next layer for
feature extraction.
Neelgagan
Neelgagan
No.
Date
No.
Date
Feature Extraction in (NN.
• The first layer usually extracts basic features such as
horizontal or diagonal e edges. This off is passed
on to the next layer which detects more complex features
edges.
suchos corners or combinational
As we move
deeper into the neturit, it can identify
wen more complex features such as
objects, fous, et
ConvNets are feed-forward network that process the imput
dats in a single pass.
• Based on the activation.
scores
map of the final convolution layer
In classification layer outputs a set of confidence
(values b/w 0 and I) that opecify how likely the image is to
belong to a "class"
Gradient descent is commonly used as the optimization
Algorithm dow during training to adjust the weights of the
imput layer and subsequent layers.
What is a pooling layer?
Similar to the Convolutional tayer, the pooling layer is responsible
for reducing the spatial size of the convolved feature. This is
to decrease the computational power required to process the
data by reducing the dimensions.
There are two types of pooling: average
D
average pooling
@max pooling.
Max poli
We find maximum value of a pixel from a portion of the image
covered by the kernel.
• It also performs
as a Nous suppresant, discards the noisy
activations altogether and also performs de-noising along to with
dimensionality reduction
①Average pooling:
• returns the average of all the values from the portion of the
image covered by the Kernel.
Note: Max Pooling performs a lot better than average pooling.
20 30
Max pooling
Nealgagan
Neelgagan
12
20 30 0
812
2
0
IL
31
34
70 37 4
13 8
112 100 25/12,
average booking
79
20
Fully connects
Neal Netun
4-77
with lo
P
Flattened
2 channel
nchang
(8x8x12) (7xuenes
2
Architecture of convolution Neural Network. (CNN):
convolution
Conv
(5x5) hernet
valid padding
Max-poolin
(212)
Kernel
valid, paniy
Convolution
Conv-z
(x5) k
New Connected
ReLU actuation
Pc-3
Fulls
Neelgagan
(28×28×1)
Channel
(24x24x1
Channel
(12x1221)
14
Neelgagan
No.
Date
No.
Date
Components of CNN:
① Convolutional Layer:
It tries to learn the feature representation of the inputs,
whither it be the images of isto vo dogs or digits.
- Cor computing the different feature maps, it is composed of
several kernels/ matrix which are weed.
• So, a filter / larmel of (non) matris depends on the type of
problem we are solving, and then it is applied to the iff
data (or image) to get the convolutions feature.
• This convolution features is then passed on to the next layer
after adding bias and applying any suitable actuation function.
Pooling Layer
• The porting layer is placed b/w the convolutional layers.
• used for achieving shift invariance which is achieved by decreasing
the resolution of the feature maps.
reducing the no of connections b/w convolutional layers,
the computational burden out on the processing units
①Fully Connected Layer
lowes
No.
Date
No.
Key components of CNN.
1. Convolutional Layer
multiplications
filters / Keend: Small matrices (eg, 3x3 or 5X5) that
slide over the input image, performing element wiss
and summing up the results to produce a single value
This process extracts features from the imput data
• Stride: The no of pixel by which the filter
4/pimage.
"Astrid of 1 means
while stride of 2
moves across the
th
filte
moves one
pixel at a time
means the it mores 2 pixels at a time.
5
mean no
Date
• Padding Adding extra pixels around the words of the app
image to contred the spatial dimensions of the ofp.
"" Valid " padding
。 padding, whil "same" padding means
padding the input so that the off size matches the imput
size.
2 Actuation function
Poding Laye
4. Fully Connected Layer
• After surial convolutional and fooling layer, the high-level
reasoning in the neural network is done via fully connected
Layers
these layers
networks, where each neuron is connected to
previous layer.
J.
are similar to those in traditional neural
every
neuson in the
"The outfout of the last convolutional or pooling layer is flattened
inte a ID vector and fed intoons or mor fully connected laye
5. Off layer
Neelgayan
Neelgagan
All CNN Architectures :
t. Le Net-5 (1998)
2. AlNet (2012)
3 nha Net (2014) VGG 16
4.
Google Net (Inception) (2014)
5. ResNet (2015)
6. Dense Net (2014)
VGG17
1. Mobil Net (2017)
8.
Shuffle Net (2017)
9.
Squeeze Net (2018)
10. Efficien Net (2019)
11. Xuption (2016)
12. Na NASNet (2018)
13. Reg Net C 2020)
14.
ConvNexTC2022)
No.
Date
No.
Date
*Trangler Learning
•
The reuse
of a pre-trained model on a new problem is known as
transfer burning in ML.
A machine uses the knowledge learned from a prior assignment
to increase prediction about a new task in transfer horning.
The knowledge of
already trained machine
Learning model
is transferred to a different but closely linked problem throughout
transfer barning.
• For example, if we trained a
whether
simple classifies to predict
an image contains a backpack, we would use the
model's training knowledge to identify other objects such as
sunglasses
Transfer learning involve using knowledge gained from on
task to enhance understanding in another. It automatically
shifts weights from a network that performed task B to a
not work performing task A ...
Steps in transfer learning
1. Choose a pre-trained model
.
4.
Sehet a model that has been pre-trained on large and divers
dataset (e.g. imagiNet)..
Common pre-trained model include VGC, ResNet, Inuption
and Efficient Net.
Neelgagan
Neelgagan
2. Remove the Output Layes:
No.
Date
• The final layer of the pre-trained model is typically
specific to the original task (e.g. classifying 1000 ty
ImogeNet classes)
.
Remove this layes the replace it with a new.
tailors to the new task
3. Freeze the Pre-trained layers:
"
them
layy (1)
Optionally, freeze the wrights of the earlier layers to prevent
from being updated during & training on the new task.
Freezing layers helps retain the useful features learned from
the original dataant
4. Add
4
hew
Layers
Add new layers to the model that are
opecific to the new
task (e.g. a new fully connected layer with the appropriate no of
outfut units for the new classification problem).
5. Train the new model:
•Train the new
layers on the target dataset while keeping th
pre-trained layers frozene
optionally, fine-tune some or all of the free-trained layers with
a lower learning rate to adapt them to the new task.
1. fine-tuning:
No.
Date
| Meaming! It refers to the process of taking a pre-trained
model and making omalladjustments to sto parameters
by continuing the training process on a new, often smally.
.
datast
• Unfreeze some of the earlies layers and continue training
with a smally barning rate to fine-tune the entire model.
This step cam help improu performanu by allowing the
pre-trained features to adapt slightly to the new task.
Advantages:
• Reduced training time
Less data required.
faster convergenc
D
Improved performanc
High accuracy
cost effective
Overcoming limited data.
Enhanus dato
augmentation
Leveraging state - of art models.
increment learning
•Cross-domain application.
Neelgagan
Neelgagan
Image Ne
Limitations
Lo competit
Domain Mismatch.
Overfitting
.
Model Size
:
No.
Date
Pre-trained models are
usually large, making
them computationally intensive and difficult to deploy
On bousu-constrained device
No.
Date
fooling
Aftiste 2 fully connected layers
"At last, a softmax classifies which classifies the images inte
respective class.
.
Dependency
en
2.1. LeNet-
•
4
by-trained modb.
LeNet-S is one of the earliest pre-trained models proposed by
Yann Le cum and other in 1998, They used this architectur
for recognizing the hand written and machine-printed
characters.
The main reason behind the popularity of this model was
simple and straight forward architecture! #
its
It is a multi-layer convolution neural network for image
classification.
Architecture of the model:
The netwal has 5 layers with learnable parameters and henc
named Lenet -S-
3 sets of convolution layers. with a combination of average
Nepagan
Input
32.X32X1
The input of to this model is a 32x32 grayscale image him
the no. of channels is 1.
Convolution
Op shape
((32-5+1)x(32-5+1)
x6)
Input: 32x32x1
Feature Map
28x28x4
2
(28x28x6)
We then apply the first convolution operation with the filter size 5x5
Neelgagan
No.
Date
and we have bouch filters. As a result, we get a feature map
of size (28x2886). Here the number of channels is equal to
the no of filters applied.
Convolution
(5x5)
subsampling
Z/P
(32x32x1)
Feature Maf
Feature Ma
(14X14X6)
(28x28x1)
No.
Date
Next, we have a convolution layer with 16 filters of Dizessi.
Again the feature map changed it is 10x10x16. The output siye
to calculated in a similar mannes After this
we again
applied
am
average pooling or subsampling layer, which again
reduce the size of the feature map by half ive. 5x5x16.
Convolutio
5x7
Subpply
Convolution
subsampling
Conv
lation
(5x)
J
After the first pooling operation, we apply the average pooling and
the size of the featur map is reduced by half. Note that,
the no. of channels is intact.
Convolution
CY
subsampling
Convolution
(5XV)
32x32x1
Feature Ma
(28x2946)
Feature Map
(14X14X6)
Feature Map
(10x10x167
Featur
Ma
featur
Ma
32×32×1)
28x28x6)
M
Featu
Featu
MY
120
(14X14X6) (10X18X16) (516)
+ Then we have a final convolution layer of size 5x5 with 120
filters.
Leaving the feature map size 1X1X120
result is 120 values.
with
After which flatten
After these convolution layers, we have a fully connected layer
righty-four (84) nemons. At last we have an off
layer with 10 neurons sing the data have 10 classes.
On next page, here is the final architecture of the Lenet -5 model.
Neelgagan
Neelyagan
LeNet Architecture
Convolution
Convolution
Subsampling
Subsamplify
convolution
Inpul
32x32x)
Featur
tow
9x84x67
Fully
lay
20
20
Map
14414x1
Featu
Featu
fou
Feat
dow
10x10x45x5x16
Neelgegan
No.
Date
Architecture Details
Layer
# filters/ filter
Stride
neurons
Size
Siyd
Feature Map
Actuation
Funchen
32x32X1
Imput
Conv
6
5x5
1
28x27x6
tamh
Aug. pooling
2x 2
2
14X14X6
Cem 2
16
5*5
1
10x10 XIL
tannh
Aug. pooliny 2
Conv 3
2x2
2
SX5XIL
120
5x5
L
120
tamh
24
tanh
10
softmax
Fully connected)
Fully connected 2.
Advantages
Simplicity, straight forward
Computational efficiency
• effective for small datasets
Limitations
:
• Limited depth
• Small filter size
Lack of modern techniques.
•Scalability issues
• performance.
No.
Date
Calculation
n+ap-b+1
stridi
2.2 AlexNet Architectur
f-filtersizs
budding D
No.
Date
• Alex Net won the Image Net large-scale vional recognition challeng
in 2012.
• The model was profed in the research paper by Alex Krizhevsky
3
4
4
and his colleagues.
In this model the depth of the network was increased in comparison
to Lenet-5
Alex het had eight layers with learnable parameters.
model consists of five layers with a combination of max
posting followed by 3 fully connected layere and they use
Relu actuation in each of these layers except the ofplayer.
They found out that using the relu as amactivation function
accelerated the speed of the training proux by almost six times.
Also used the dropout layers, that prevented this mode
from overfitting.
Dataset used Imaginct dataset
Alix Net architectus:
4 contains almost 14 million images
across thousand lasse
Sinu AlexNet is a deep architecture, the authors introdund
padding to prevent the size of the feature maps from redeving
dashially
input to the model:
Layer #filters / Filter
Imput
neuson
-
No.
Date
O/Pz Input-filter size + T
Dide
227822783
Output(Inputfilterical
Stride
Stride Padding Size of
Activation
Size
Learn Mop
function
227x227x3
Convl
96
Y
55x55x96
ReLU
MaxPool
-
3x3
2
27x27x91
Conv2
256
5x5
2
27x27x256
ReLU
Max Poul
3X3
2
13X13X256
Conv3
387
3x3
13X13X384
ReLU
Convy
394
3×3
13X13X384
ReLU
Convs
MaxPods
256
3X3
13X13X257
ReLU
383
2
6x6x256
6x6x256
4096
ReLU
4096
4096
ReLU
1000
Dropout rate=0.5
Fully Connects
Propout Mateor
FC 2
fc 3
SoftMove
•If we look at the architecture till now, the no. of filters is
increasing as we are going duper. Henu it is extracting.
we move deeper into the architecture
mou
features.
as
Neelgagan
No.
.
Date
•Also the filter size is reducing, which means the initial
filter was large and as we go ahead the filles size
is decreasing resulting in a decrease in the feature map snake.
Advantage
•
'
4
4
.
⋅
Depth and complexity
ReLU activation function
Dropout for Regularization (To combat overfitting)
Date Augmentation cam be employed
Local Respons. Normalization (LRN): It used LRN to
normalize the output of each meuson across the feature map
which helped in reducing overfitting and improved the
model's performanc
GPU utilization
Limitations
• High computational cost Clarge no. If parameters lead to memory was)
Limited depth (relatively shallow compared to modern architectures
Large filter size
4
3
• LRN : LRN is no
longer commonly used in modern architecture
because it das not provide significant benefits and can be
Computationally expensive. Batch normalization is typically
preferred for its regularization and performance benefits.
Lack of Modulas Design
Manual Hyperparameters
2.3 VG Net
No.
Date
It is a typically deep convolutional Neural Network (CNN)/design with
numerous layers.
vaa stands for Visual Geometry Group.
The term "deep defines the no. of layers, with VGG-16 Or
VGG - 19 having 16 as 19 convolutional layers respectively.
• Innovative objective identification models are built using the
VaG architecture.
VG Net
Layer1
Layer.
Conv
Conv
Layes
Conv
Conv 17
Layer
Conv
Tony
Layes Por
[[FC]
PET
Layes 6
FC
Lay Soft Max
Neelgegan
Neelgagan
No.
Date
No.
Date
.
•
VGG16
The convolutional neural network model called the VGG model, a
VGGNet, that supports 16 layers is also known as BVG616.
It was
developed by A. Zisserman and K. Simonyan from
The University of Oxford.
"It significantly outperforms Alex Net by substituting
several 3x3 hermal-sized filters for the huge kunel-sized
filters.
The VaG Net 16 has 16 layers and can classify photos into
100 diffrent object categories,
• The model abo accepts images with a resolution
2246 229.
•
VGG19
The VGG model Cabo known as VGG Net 19) has the
same basic idea as the VGG 16 model, with the exception
that it supporto 19 layers.
The number '16" and "9", refer to the model's 'wright layers
(convolution layers).
In comparison to VGG 16, VGG 19 contains three extra
convolutional layers.
752
12
Neelyngan
224x224 Input
224224 3x3 Convt ReLU 64
3x3 Ceny ReLUBY
(2x2 Max-Pool-64)
3x3 Conv RLV 128
2x2 Maxfool. 128
112X112 3x3 Conv +Real 128
3x3 ContReLU 256]
3x3 Conv ReLUS
56x56 3x3 Conv +ReLU 256
2x2 Max-Pool 256)
3K3 ConveReLU-512]
3x3 Conv+ ReLU SIL
23x18 3x3 conv + ReLU 512
Pool 512
14x14 3x3 ReLU 512
3x3 Conv +ReLUS12
Max-Pool 512
7x7 FC 4096
IFC 4096
FCO/P1000
Size:224
Size: 112
| Imput
3x3 Conul, 64
3x3 Conv267
Pod/2
3x3 Con 3, 128,
3 x3 cenu 7, 127
3x3
Size: 56
Size: 28
Size:14
Size: 7
Pool/2
Conv 5 256
[3x] Conv
3x3
5x Convo 254]
13x3 Convt L
(3x) Cemu 12
343 Con 12/512
3x3 Conv
Pool/2
3x3 Conu 135/L
33
3x3 con 14 512
13x3 Lou 15,512
Pod
4096
fc 4091
16.1000
Neelgagan
No.
Date
No.
Date
Vicent
Comparison b/w VGG-16 and vac-19.
Featur
Total Layers
fully connected
Max pooling layer
rac-16
16 weight layers
33
S
2x 2
VaG-19
19 weight layer
+63
2x2
Layes
5
Convolution filter
3x3
3x3
size
Poding layersize
No-parameters
~133 million
~144 million
(2+2+414+4)
Input image size
224x2243 (RGB)
Actuation n
Flayes
Output laye
224x224XRB
13
extraction
16
Depth Configuration (2+2+3+3+3
Convolution laye
Typical Uss loss
Relvafter each Convand PC ReLU after each conv and
1000-way Softmar
1000-way soft max
Image Classification feature Image Vassification, feature extraction
High acumaly, suitable
Higher than V GG-16 due to
more parameter.
Relatively high due to Higher than NGG-16 due to
depth and parameter more parameters.
Very T
Performany
for transfer learning
Training time
Computational Cost
T
Memory Usage
1-
Model Siz
Deployment
Large
Suitable for or cloud
deployment
Suitable
for h
PU OA
Neelgagan
Very T
Very Large
cloud
deployment
.
62 Google Net
Inception Modul
• The inception module is a building block for convolutional Neurd
Networks (CNN) introduced by Google researchers in their
seminal paper "Going Deepes with convolutions" in 2014.
•This architecture, also known as Google Net, represented a
The inception module is designed to allow a CNN to benefit from
multi-level feature extraction by implementing
4
various sizes in the same layer of the network.
Key features:
Multi-level feature extraction
Dimensionality reduction
• Pooling'
.
Evolution
features of
The inuption module has evolved through several ituation,
leading to improved versions such as Inception 12, Inceptions
and Inuption -14.
These versions have introduced various optimizations,
including factorization of convolutions, expansion of the filter
bank outputs, and the use of residual connections.
One notable variant is the Incipition -ResNet hybrid,
combines the inception architecture with residual connections
which
Neelgagan
No.
Date
No.
Date
from ResNet.
Google Net
• Google Net, released in 2014, oct a new benchmade in object
classification and detection through its innovative approach
(achieving a top-5 vera rate of 1.1%, nearly half the
•
veros rate of the previous year's winner. ZF Net with 11.7%)
Googl Net's deep learning model was defer than all the previous
mocks released with 22 layers in total
• Increasing the depth of the Machine Learning model is entupir
as duper models tend to have more learning capacity and
as a result
this increases the performance of a model.
Only posible if we solve vanishing gradient prostem.
9
The inception module, the key innovation introduced by a
team of Google researchers solved this problem. Instead of
deciding what filter size to use and when to perform a
pooling operation, they combined, multiple convolution
max
filters...
Neelquyun
Detailed Inception Module Explanation
The inception module is a key innovation introduced in the
inception (GoogleNet larchitecture. It allows the neturach to
efficiently captors and process info. at multiple scales by
applying several different types of convolutional and pooling
operations in parallel.
Structure of
an
Inception Module:
An inuption module typically consist of the following components-
"
IXI Convolution
• Reduus the no. of input channels (dimensionality reduction,
before applying more computationally expensive. Convolutions
(like & 3x3 and 5X;). This helps to reduce computational
cost
343 Convolution:
• Captures medium-sized features from the input feature map.
Before applying a 3XI convolution, a (X) convolution is often
used to reduce the number of input channels, which makes
the process more
efficient
3Sxi Convolutions
Captures larger features from the input feature map. Similar to
Neelgagan
No.
Date
.
3x3 convolution, a IXI convolution is applied beforehand to
reduce computational expense.
3x3 MaxPoding
"Helps in capturing spatial relationships and invarianc
max
pooling
a
I convolution is applied to the
After
pooled feature map to control the dimensionality
Concatenation:
The outputs of the above operations and are concatenated along
the depth (channel) dimension to form the final outpute
of the inception module. This results in a combination of
feature maps at different scales.
Nelgagan
Architecture
tecture of Google Net
No.
Date
type
patch o/poize depth #11 #3x3
piy/stride
Convolution 7x7/2
redu
reduce
#3×3 #5x5 # 5x5 pod ops.
pros
112X1124
34M
maxpool 3x3/2
56x56x14
0
Convolution 3x3/1
56x56x192
2
64
192
31OM
maxPool 3x3/2
28x28x/92
inuphora)
28x28x256
Inception (35)
28x28x420
722
64
96
128 16
32
32
122M
128
128
192
32
6
91
64
304
maxpool 3x3/2
14X14X430
0
inception (7a)
14X14X512
2
192
91
208
IL
24
(4
73 M
inciptionly by
14X14X512
2
160
112
224 24
64
164
88 M
inception res
14X1YKSIL
2
120
122
256
24
by
64
loot
inception
Md
112
144 29
32
6y
64
1191
Thuphin (re)
14X17X832
2 25 120 320
32
123
122
1701
marpool 3×3/2 7x7x832
0
inuptional
7X7X832
2
251
160
320 32
128
127
SUM
inception (5)
2
7x7x1024
384 192
384
42
128
21
aug pool 7x7/1/1X1X1024
D
dropout
XIX1024 O
(40%)
linear
IXIX/000
IM
tmax
D
IXIXION
Neelgagan
No.
Date
of Inception
• GoogleNet model. is particularly well-known for its use
modules, which serve as its building blocks by using parallel. convolutions
with various filter bije (vel, 3x), and 5x5) within a single
layer.
The outputs from these filters
are concatenated.
$
Moreover, the architecture is relatively dup with 22
layers, howevy
the model maintains computational efficiency despite the
increase in the no. of layer.
Key features of Google Net
• Inuption Modul
°
•
The X1 Convolution
Global Average Pooling
Puxisory Auxiliary. Classifies for training
Filter concatenahon
3x3 Conv
SKS Conv
1X1 Conv
T
Ix Conv
1X1 Conv
1x1Conv
3x3 Mar Pooling
•Input featur Map Size: 28x28.
•Input Channels (0) 192
• No. of filtus in 3x3 Conv (F):96
# (lobal Average Pooling
C
No.
Date
It is a CNN technique in the place of fully connectal layer
at the end part of the network. This method reduces the total
no. of parameters and minimizes overfitting
# Auxiliary Classifiers for Training
"These are intermediat, classifier found on the side of the network.
These are
only used during training and in the inference, these are
emitted
Auxiliary classifies help overcome the challenges of training very
Deep Neural Networks, and vanishing gradients (when the gradients
turn into extremely small values).
featur
• In Google Net architecture, there are two auxiliary classifiers in the
network. They are placed strategically, where the depth of the
extracted is sufficient to make a meaningful impact, but befor
the final prediction from the off classifice.
Neelgagan
Previous Layer
Neelgagan
No.
Date
No.
Date
Structure of each auxiliary classifier.
.
An
average poding layer with 5x5 window and stride 3.
• AIX) convolution for dimension reduction with 128 filters
.
Two
connected
layers
the first layer with 1024 unite,
fully
followed by a dreport layer and the final layer corresponding.
to the no. of classes in the task.
Apoftmax layer to o/p the prediction probabilities
+ There auxiliary classifiers help the gradient to flow and not
diminish too quickly, as it propagates back through the staf
dufer layer. This is what makes training a deep neural
network like Google Net possible.
→ Moreover, the auxiliary classifiers also help with model
regularization. Sinu each classifies contributes to the final
output, as a result, the network distributes its learning
different parts of the network.
acron
→ This distribution prevents the network from relying too heavily
on specific features or layers, which ruduus the chances of
ourfitting.
Neelgagan
Inception Family Varianto
Google Not (Inception V₁)
introduction of inuption modules, auxiliary Jassifics,
global average pooling
2014
V2
2. Inception v2
Batch normalization, factorized convolutions (eg, 5x5.
repland by two 3×3), improved training
peed
2015
3. Inception v3
"Further factorization (eg., (x7 and 7x1), asymmetric
convolutions, label smoothing
-
2015
4. Inception vs
Increased depth and
techniques
5. Inuption - ResNet V,
and complicity, advanced factorization
Combination of inception modules with residual connections
Enhamud version of Inception Respet VI with further improvement
Neelgagan
No.
Date
No.
Date
Vanishing Gradient Problem:
The
Vanishing
many
Gradient Problem is a common issue in the training
of dup neural networks, particularly those with
layers. It occurs when the gradients of the loss function with
respect to the network's parameters become very small
as they propagated backward through the network during training.
This makes it difficult for the network to learn and update
its parameters effectively.
#Causes:
1. Activation functions like sigmoid and tank can squash their input
Anto
avery
small range
2. Deep Networks: the multiplication of many small gradients
during backpropagation can lead to an exponentially small gradient."
3. Weight Initialization:
•
Poss weight initialization cam exacerbate the vanishing
gradient problem.
If weights are initialized with very small values, the
gradient
com
quickly become too small to be useful.
# Consequences:
very
vuy
alow because
Amal
.
1. Slow convergeny Training becomes
the updates to the network's parameters an
2. Poor Performance: The network may fail to barn verful
features, especially in the early layers.
#Solutions!
1. Activation functions
2. Weight Initialization
ReLU and it's varianto.
•He Initialization: For ReLU and it's varianto, He
initialization (random initialization) from a distribution
with zero mean and variance scaled by
units) helps maintain the gradient flow.
the mo
mo. input
• Xavier Initialization: For sigmoid and tamh activation
functions, Xavies initialization helps by scaling the weig
according to the no. of input and output units.
3. Batch normalization
4. Residual Connections
a
ResNet
· Highway Networks.
wrights
Neelgagan
Neelgagan
No.
Date
No.
2.5 ResNet Architecture
• ResNet stands for Residual Neural Network and is a type of
convolutional Neural Network (CNN).
C
It was
designed to tackle the issue of vanishing gradients
deep networks, which was a
deep neural networks.
majo
hindrance in
in
developing
The ResNet architect us enables the network to learn multiple
layers of features without getting stuck in local minima, a common
issue with
deep
Features of ResNet:
.
•Residual connection
networks.
as the
Identity Mapping ResNet uses identity mapping
residual function, which makes the training proass easier
by learning the residual mapping rather than the actual mapping.
Depth: ResNet enables the creation of very deep neural networks,
which can improve performance on image recognition
fash
Fewer parameters: ResNet achieves better results with fever
parameters, making it computationally more efficient
State-of-the-art Results: ResNet has achieved state-of-
the art results on various image recognition tasks and
has become a widely
used benchmark for image recognition
tasks
How ResNet works?
au
otached
Date
to forma ResNet.
• Many residual blocks.
together
We have "shipped connections" which are
ResNet
9
the
major part of
The idea is to connect the input of a layes directly to the
output of a layer after skipping a few connections.
Weight Layer
f(x)
relu
Identity
(Weight Layer)
{(x)+ x Plus
to
directly
Here, x is the input of the layer which we are
using
to connect to a layer after shipping identity connections
at the identity connections and if we think the output
from identification connection to be f(x). Then we can
say that output will be f(x)+x.
Neelgagan
Neelgagan
•
4
A
No.
Date
and f(x) may vary
• One problem that may happen is regarding the dimensions.
Sometimes the dimensions of x
and this needs to be solved.
4
Two approaches
can be
followed in such situations:
1. involves padding the input x with weights such as it
of the
coming out.
how brought equal to that
2. include using a convolutional layer from x to addition
to flx)
value
The skip connections in ResNet solus the problem of
vanishing gradient in deep neural networks by allowing this
alternate shortest path for the gradient to flow through
The complete idea is to make F(x)=0. So that at the
end we have Y = X as result. This means that the
which
value. coming out from the activation function of the
identity blocks is the same as the input from
shipped the connections.
ResNetso
we
ResNetso is CNN architecture that belongs to the Res Net
(Residual Network family, a series of models designed to
associated with training deep neved
addres the
networks.
Challenges
• Developed by researchers at Microsoft
Neelgagan
No.
Date
•ResNet architecture comes in various depths, such as ResNet-12,
ResNet -32, and so forth with ResNet so being a mid-sized variant.
It w
was released in 2015
ResNet and Residual Blocks
• The primary problem ResNet solved was the degradation problem
in deep neural networks. As networks become deeper, their accuracy
saturates and degrades rapidly.
• This degradation is not caused by overfitting, but rather the
difficulty of optimizing the training process.
20
training error (y)
a
10
m
2
ity.
3
16-layer
Y
20. lages
test essor (1)
20
ires.
56-laye
20-lagu
The deepes neturer has higher training error and this test ersor
ResNet solved this problem using Residual Blocks that allow for the
direct flow of information through the ship connections.
Neelgagan
No.
Date
• The residual block used in Res Net 50 is called the Bottleneck
4
Residual Block.
26-d
No.
.
Date
a filter size of 3x3
The second convolutional layer might use a
to extract spatial features from the data
The third convolutional layer again uses a filter size of 1x1 to
restore the original no of channel before the off is added to
the short cut connection.
3. ship Connection
3x3, 4
1x1256
rulu
Breakdown of the architecture or the residual block.
1. Relly Activation
2. Bottleneck convolution Layers:
The block consists of three convolutional layers with batch
normalization and ReLU activation after each :
B
filter size
• The first convolutional layes likely mais a
of 1x1 and reduces the no. of channels in the input data.
The dimensionality reduction helps to compress the data and
improve computational efficiency without sacrificing too
much information
Stacking
the Blocks Building ResNet-50.
.
ResNet-50 incorporates so bottleneck residual blocks,
arranged in a stack manner. The early layers of
the network feature conventional convolutional and pooling
layers to preprouss the image before it undergoes further
processing by residual blocks.
ultimately, fully connected layers positioned at the pinnack
of the structure utilize the refined data to categorize the image
with prevision.
Nenigayan
Neelgagan
2-6
DenseNet
No.
Date
its destination.
No.
Date
- Demar Net is densely connected -convolutional networks.
It is very similar to a ResNet with some fundamental
differences. ResNet is using
an additive method that
means they take a previous output
9am
input for a
future layer, and in Dense Net takes all previous.
output
below
223
imput for a future layer as shown in to
10000
No of connections in Dense Net =
LCLU
2
• In dense net, we have less no- of layers than the other.
model, so hen we com train more than 100.
Totonates modt of the model very easily by using
this technique
Input
Dena Block
fooling
Leather prizes
reduce feature
Dense Block 2
layers
4
Dende Net architecture was specially developed to improv
accuracy caused by vanishing gradient in hig input
neural networks due to the
long distance b/w input
and off layers and the info.. vanishes before reaching
Neelgagan
off a
Neelgagan
Peeling
Dense Blocks
a
Convolution
Cooling
feature map sizes match within each
block
•
.
No.
Date
Ao we go deeper into the network this becomes a kind
of unsustainable, if we go 2nd layer to 3rd layer so
3rd layer takes an impout not only 2nd layes but it
takes imput all previous layers.
Let's
will
layer
Day we have about 10 layers. Then the 10th
take us to input all the feature maps from the preceding
nine layers. Now if each of these layers, let's produce
122 features. maps and there is a feature map explosion,
to ourcom this problem we create a dense block here and
so each dense black contains a prespecified no. of layers
inoid them and the o/p from that particular dense block
to what is called a transition layer and this
is given
layer is like IX I convolution followed by Marpooling
to reduce the size of the feature maps.
• So the transition layes allows for max pooling, which typically
leads to a reduction in the size of the feature maps.
B. The fig onth previous page shows the two blocks first on
is convolution layer and the second is the pooling layer and
Combinations of both asse transition layer.
Concepts in Dense Net
No.
Date
• Growth Rate: This determines the no. of feature maps off
into individual layers inside dense blocks.
• Dense Conectivity: By dense connectivity, we mean
that within a dense block each layer gets us imput
feature maps from the i/p feature maps from the previous
lages.
°
Composite functions: Sequence of operations :-
- batch normalization
· Transition layers
2.1 Mobile Net V2 :
•
1
ReLU
Convolution Layer
A light weight CNN architecture, Mobile Net V2, is specifically
designed for mobile and embedded vision applications.
The model's ability is to strike a good balance between
model size and accuracy, rendering
- Constrained devices.
it ideal
for
resousu
Neelgayan
Neelgagan
Why
128x128x3
prproaning
ReLU
3x3 Conv
4-32
Max pool
5=96
22
ReLU
3x3 Conv
No.
Date
42/290
411280
T
Resprousing
128x128
64x64
32x32
Mobile Net V2
use Mobile Net V2 for Image Classification?
4x4
①It's lightweight architecture allows for efficient deployment
on mobile and embedded devices with limited computational
resources.
It achieves competitive accuracy compared to large and
more computationally expensive modib.
model's smally size enables faster inferemetimes,
making it suitable for real-time applications.
Neelgagan
Mobile Net V2 Architecture
Depth wise separable Convolution
No.
Date
It is a technique used in Mobile Net V₂ to reduce the
Computational cost of convolutions. It separates the standard
convolution into separate operations: depth wis convolution
and pointwis convolution.
• Inverted Residuals
the model'ea
It is a key component of Mobile Net VL that helps improve
e accuracy. They introduce bottleneck structur
that expands the no. of channel before applying depth wise
separable convolutions. This expansion allows the model to
capture more complex features and enhance its representat
-ion power.
Bottle neck Design
It reduces the computational cost by using IXI convolutions
to reduce the no if channel before applying deftruise
separable convolutions. helps maintain a good balance b/w
model size and accuracy-
· Linear Bottlenecks:
the
It is used to address the issue of info.com during
bottleneck frouss By using lineas activations.instead of
non-linear activations, the model preserves more information
Neelgagan
No.
Date
and improves it's ability to capture fine-grained details.
•Squeeze-and- Excitation (SE) Block:
They
are added to enhance its feature representation
capabilities. These block adaptively recalibrate the channel-
wise feature explosis responses, allowing the model
and suppres
on more informative features
to four.
les relevant ones.
2.8 Efficient Net:
• The technique of randomly scaling model requires
manual tuning and many & person hours, often resulting
in little or no noimprovement in performane
• The authors of Efficient Net proposed scaling up CNN modib
to obtain better accuracy and efficiency
ifficiency in a much mor
·
moral
way.
and
Introduced in 2019, it became a go-to architecture for many
tacks, including object recognition, image segmentation
even language proussing.
Ability: Computational efficiency and model performan
resolution) in a
By systematically scaling the model's dimensions (width, depth and
a principled manner, Efficient Net achieves
unprecedented levels of efficiency without compromising actively
Neelyayan
•
No.
Date
"
It is build upon the concept called compound scaling"
•The idea behind compound scaling is to ocal thre essential
dimensions of neural network: width, depth and resolution
1.Width:
• refers to the no. of channel in each layer of the newral
netwo
↑ width, model Com capture more complex patterns
and features, resulting in improved accuracy
I width, led to a mon lightweight model, suitable for
low-resouse environments.
2. Depths: Dapto
•
scaling pertains to the total no. of layers in the network.
• Deeper models can capture more intricate representations
of date, but they also demand more computational resources
shallower models
sacrifice accuracy.
3. Resolution:
are
computationally
efficient
but
may
Scaling involves adjusting the imput image's size.
• ↑ resolution images provide more detailed info.,
potentially leading to better performance. However, they
abo require mon memory and computational power.
Neelgagan
No.
Date
No
consume
fewer
fine-grained details.
• I resolution images,
lead to a
los in
How compound scaling works
resourus but may
•The prouss begins with a baseline model, which serves as
This baseline model is usually a
the starting point,
Masonably sized network:
a user-defined
• Then a compound wifficient is introduced as a
parametes that dictate how much to seal the dimensions of
the neural network. It is a single scalar value that uniformly
scales the width, depth and resolution of the
By adjusting the of vale. The overall. complexity and resource
requirements of the model can be controlled.
modd
• The arding factors for each dimension are derived from
the compound coefficient &.
Neelgagan
→ Width scaling! The width of the neural network is cated
proportionally by raising & to the power of a
T
specific exponent (typically denoted as α)
→ Depth scaling: Similarly, the depth of the network
is scaled by raising & to another exponent (denoted as pe
→ Resolution scaling: The resolution or input image size
is tocaled by multiplying the original valution by
°
Date
& raised to a different exponent (usually denoted as Y)
Optimal exponents need to be determined.
+ The values of these exponents are typically derived
through empirical grid ocanh us optimization process.
• Smaller & more lightweight and resouse-efficient
modib.
larges &
more powerful but computationally
intensive modeb.
Architecture
• uses Mobile invited Bottleneck CMB Cons) layers. which
are a combination of depth-wise separable convolutions
and inverted residual blocks.
2
use
SE.
T
Input Imag
Conv 3x3
MBConv1, 3x3] Jo Alak 1
MB Convb., 3x3
MB Lenu 6, 3x3
[MB Lon√6,5x57
MB convoy, 5x5
MBConv6 3x3
Mblonus, 3x3
MB Conu 6, 3x3
MPConv6,5x5
MB conv6 SX
MBConv
FDC, 575/
MD Convé, 5x5
MBConv6, 5x5
(MB Conv 6, 5x5
MB temu, 33
Fatature Map.
Block 2
Block 3
Hi Blocks
Bloch -6.
I Block
Neelgagan
No.
Date
IOCR
OCR stands for Optical Character Recognition
-
It is used to describe algorithms and techniques (both electronic
and mechanical) to convert images of text to machine - encoded
text.
These are the systems that I
7 Accept
generated)
a
am
input image (scanned, photographed, or computer-
→ Automatically
would
detect the text and "read" it as a human
→ Convert the text to a machine readable format so that it
cam be searched, indexed, and prouessed within the scope of
lg larger computer vidion systems.
com also be mechanical and cal physical
OCR systems!
→ Digital Pens
Tesseract OCR
Cloud API
+ Custom trained models
-
OCR is hard
+ There are so many nuances in how humans Communicate
via writing
+ Different writing styles, different fonts,
Neelgagan
No.
Date
No.
Date
J
Documents that are
noisy, dirty, distorted, etc.
We have all the problems of Natural Language broussing (NLP),
compounded with the fact that computer vision systems
obtain 100% accuracy
when reading a piece of
will never
text from an image.
- History of Ock!
T
Early Ock technologies were purely mechanical, dating back he
1914 when Emanuel Goldberg developed a machine that could
read characters and then converted them into standard telegraph
code
Goldberg continued his research into the 1920s and 1930s when
he developed a system that searched mirifilm (scaled-down
documents, typically films, newspapers, journals, etc) for
Characters and them OCed them.
+ In 1974, Ray Kurzweil and Kurzweil Computer Products, Inc.
Continued developing OCR systems, mainly focusing on creating
a "reeding machine forthe blind":
+ Kurzweil's work caught Xerox's attention, who wished to commercialize
the software further and develop OCR applications for document
understanding
T
Hewlett-Packard (HR) Labo started working on Texeral in the
19805
HP's work was then open-sourced in 2005, quickly becoming
the world's most popular OCR engine
Tesseract library plays important part in OCR
+ AD Deep Learning revolutionized the field of CV in the 20100,
OCR accuracy
was given a tremendous boost
from specialized architectures called Long Short Term Memory
CLSTM) Network
Now in the
20200
we're
acing how OCK has become increasingly
commercialized by tech giants such as Google, Microsoft and
Amazon, etc.
→ We exist in a fantastic time in the computer science
field
still take a
But the fact remains that these OCK engines
knowledgeable.computes vision practitiones to operate
- Applications of OCR =
"Automatic license/ number plate recognition (ALPOR/ANPR)
I Traffic Sign Recognition
Analyzing and dobry defeating CAPTCHAD on websites.
Neelgagan
No.
Date
+ Extracting information from business cards.
Automatically reading the machine-readable zone (MR2) and
other relevant parts of a passport
J
+ Parsing the routing number, account number, and curreny.
amount from a bank cheque
+ Understanding text in natural sumes such as
captured from the smartphone
- Orientation and Script Detection (OSD)
the photos
of och is the process of taking an input image and returning
the text in both human-readable and machine-readable
formats, then OSD is the process of analyzing the image for text
I meta-data, specifically the orientation and script /
ww
tingstyl
+ The text's orientation is the angles in degrees) of the text in an
input image
OSD to
+10 obtain highes OCK accuracy, we may need foot fly OCR.
determine the text orientation, it and
→ Script and Writing styl refers to a set of characters and symbols
used for written and typed communication
→ Most of us are familias with Latin Characters, which moke of
the Characters and symbols werd by many European and Western.
countries; however there are many other forms of writing styles
widely used, including Arabic, Hebrery Chinese, &c.
that are
No.
Date
Latin Characters are very different from Arabic,
which is, in turn, distinct from Kanji, d'oystem of Japanese
writing the using Chinese characters.
→ Any rules, heuristics, Orassumptions an OCR system.com moke
regarding a particular script is writing system will make the
engine
that much more accurate when applied to a given
-
ock
script
Therefore, we may use OSD information as a precursor
to improving OCR accuracy
The importants of Post - Prousing and Pre-Prousing
→ Many OCR engines (whether Texeract or cloud-based APID)
will be more accurate if we com
we can apply computer vision and
image processing techniques to clian of the image.
T
If we were to apply some basic image processing
operations such as thresholding, distance transforms and
morphological operations, un would end up with a lear image
When we pay the chaned up image through Tesseract,
we will (ideally) obtain more accurate results
Nealgrynn
Neelgagan
that they
assume
No.
Date
the OCR
7 One of the most common mistakes computer vision and deep
learning practitioners make is
engine they are utilizing is also a generalized image processes
capable of automatically cleaning up theirs images.
T
s
Thanks to advances in the Tesseract Ockengine, Ock systems
can conduct automatic segmentation and page analysis,
howwe these systems are far from being as intelligens
u-instantly parse text forma
humans, who cam near-
from complex background.
OCR engines should be treated like 4th graders who
capable of reading text and meed a nudge in the right
direction quite often
are
+ It's far lasies for an OCK system to recognice apica of text
of it is preferly cloned and segmented first.
+ We should also consider post-processing the OCR'd text.
→ OCR systems will never be 100% accurate, so we should assume
there will be some mistakes.
To help with this, ask yourself if it's possible to apply rules
and heuristics - some examples.
examples to consider:
No.
Date
1. Com I apply automatic spellchecking to card correct
mispelled word from the OCR proun?
Can I utilize regular expressions to determine patterns in my outful
OCR date and extract only the information I am interested in
Cie, dates or prices from an involu)!
Can I leverage my domain knowledge to create heuristics
that automatically correct OCR'd text for mit
OCR tools and be API.
→ Tesseract Ock engine, the most popubs Ock package in the
T
wordd
bython and bytesseract library
Tesseract
com mole an
inferences with
impact computer vision and image processing algorithms com
have on the accuray of Ock.
- cloud-based APIs
OCR tools and libraries:
To interact with the Tesseract OCK
using bytesseract
engine
via lythen, we'll be
- Tesserad and pytesseract are not enough by themselves. OCK
accuracy tends to be heavily dependent on how well we can
"clean up" our input images, making it easies for Teerauto ock them.
- To clean up and pre-process the images, wi'll use Opener,
+ ML and DL libraries: scikit-learn, scikit-images, Keras, Tensorflow
Neelgagan
Neelgagan
No.
Date
No.
Tesseract
7 The Tesseract Ock engine was first developed as close source
Hewlett-Packard (HP) laks in the 1980.
by
softwar
+ When Tesseract was initially developed, very little was done to
update the software, patch it, as include new state-of-the-art OCR
algorithms
+ Tesseract while still being utilized, esentially sat dormant until
it was open-soused in 2005.
* Google then started sponsoring the development of Tesseract in th
-
+
2006
fetare feature
came in October
a new
the most prominent new
2018 when Tesseract v4 was released, including
deep learning OcKengine based on Long Short-Term Memory
CSTM) networks.
→ The new LSTM engine wast rained in over 123 languges,
making it easier to OCR text in languages other than English
(including script-based languages such as Chinese, Arabic, etc)
Py Tesseract
It is a python package developed by Mathias Lee, a PhD in compute
Duente
Date
Py Tesseract library is a python package that interfaces with
the tesseract command line binary-
Cloud OCK APID
There will be times when simply no amount of image processing/
Cleanup and combination of Tesseract options will give me accurate
ock results:
7 Perhaps Tesseract was never trained
image
on
the fonts in the input
+ Perhaps no pre-trained "off-the-shell" models can correctly
localize text in the image
1 Or maybe it would take too much effort to develop a custom
3 OCR pipeline and we're instead looking at for a shortet.
When these types of seem senarios present themselves, we should
consider usily.
cloud-based OCKAPI, such as Microsoft Azure
Cognitive Servius, Amazon Relegnition and Google to cloud
Platform. (GCP) API.
The APID are trained on massive text datasets, potentially
allowing us to accurately.OCK complex images with a fraction of
the effort
Neelyagan
Neelgagan
No.
-
Objed Detection
What is Object Detection ?
Date
- Object Detection, within computer vision, involves identifying
Ebjects within images orvideos. These algoritme commonly
rely on machine learning as deep learning methods to
generate valuable outcomes.
wa
- for example in set of dogs image
Vassifying, which type of dog is present,
actually locate a dog in the image
1
We com vcate a
in the
of this box
present
instead of
we have to
box
and
specify
the x and
around the dog that is
coordinates
b
y
bounding
image
so, the box around the object representing the coordinates.
of the boxes that shows the location of the object is known
bounding box
- The abow type of problem becomes an image to futer
fudle localization problem
-
We can have single as well as
multiple dance
4
When we need to clarify the image objects in the images.
then it becomes object detection.
In the case of the object detection problems, we have to tarify
Jassify the objects in the image and also locate where thear
objects are present in the image. But the image classification
Neelquyun
have
No.
Date
problem kadonly one task where we have to classify the
objects in the image
0
Image Vassification
->
* Object classification
Why Object Detection Matters.
• safety
Driving
shopping
Healthcar
°
• manufacturing
How Object Detection works:
Object Detection
Object Localization
Object Classification
Looking at the picture: Imagine a completes looking at a picture
• Finding clues. The computer looks for clues like shapes,
colors and fatterns in the picture.
D
Guessing what's ther: Based on those clues, it makes guess
be in the picture
about what
might
Checking the guesses: It the ches each gues by comparing
to things it already knows.
it
Drawing boxes: If it's pretty sure about something, it draws
abox around it to show where it thinks the object is.
• Making sure! Finally, it double-chucks if the its guesses to
make sure it got thing right and fix any mistakes.
Neelgagan
Training dato for Object Detation.
Object Detection problem tasks.
To an Object present
image?
in the
(90)
No.
Date
What is th
When is the imag
located?
Object?
X
min
Ymin
Xmax
y
Y/p Emagi
In this
Case
Target Value
target voters variables have five values the valus
p denotes the probability of
an
in the abou
object being
Xime
image when as the four value Xmin, Ymin, X and Y mes
denote the coordinates of the bounding box and xmax and
Neelgagan
No
Date
Ymax will be the bottom right corner of the bounding box
What happens when there are more classes?
Day
am
So, if we have 2 classes which are at leth
emergeny
vehick and non-emergency vehick, we'll have additional
value clandes denoting which class does the object present in the
image
10
So if we conside one example image in which there is an emergeny
vehicle present then, we have the probability of am object
present in the image as me. I.
We have the giver Xmin, min, & max, Ymar
of the bounding box.
And then we hav
= /
CI
as the coordinate
sinuit an
emergeny
vchill
and (2=0 . because there's no non-emergency vehicle present in
the image
re
1
0-8
40
7°
min
Ymin
20
30
Xmax
210
220
(20
190
Ymay
cl
C2
0
0
Target values.
1/p target
Predictions
Neelgagan
No.
Date
No.
Date
Bounding
Box Evaluation - Intersection Over Union (IU)
Jex)
Predicted Bounding Boxes
Box
Original Rounding Fox
+
which bounding box more accurate I
will be able to make a decision as to
Very obvious answer would be box, but why?
If we are able to find the overlap of the actual and the predicted
bounding boxes, we
which bounding box is a better prediction.
♡
fox
Bonz
So the bounding box that has a higher overlap with the actual
bounding box is a better prediction. Now, this overlap is called
the area of intersection for this & Box,. We can say
area of intersection is about 80% of the actual bounding b
Whereas Box2
So, w
we com
Day
But having
Sumario-1:
is about 20%
Box is a better prediction.
that
• on the area of intersection alone is not enough. Why?
Let's suppose we have created multiple bounding boxes or patches
of different sizes.
Aree of
a
intersection = 100%
Area of intersection = 70%
So, at this stage, would you say that the bounding box having
Area of
intersection = 100% is a better prediction? Obviously not
Neelgagan
Nelgayan
No.
Date
No.
Bounding
Box on
Might
is more accurate
of union,
bounding boxana
So, to deal with such sunaries, we also consider the area
which is the patch area, as well as the
Ioll = Aree of intersection
Aria of Union
Note: Higher the area of union & we can say that less accurate will be.
predicts bounding box, or the particular patch.
the
This is known as Intersection over Union.
ZOU =
The
range of
Area of Intercon
Area of Union
the Iou on Intersection over Union is between 0 and 1.
Date
We often consides a thrushold, in order to identify if the predicted
bounding box is the right prediction..
So let's
say if the ZOU is greater than a threshold value
which cam be, let's say 0.5 020.6. In that case, we will consides
that the actual bounding box and the predicted bounding box are
quite
similar
Area of Intersection
Area of union
threshold
Day
Whereas if the Iol is less than a particular threshold, we'll
that the predicted bounding box is nothing close to the actual
bounding box.
Area of Intersection
(0,0)
(xmar, ymas)
Neelgagan
Neelgagan
No.
Date
So, the coordinates will be Xmin, Ymin, Xmax and Ymax
using these coordinates values will be easily able to
of intersection
calculate the area
In order to find out the value of XMin, we are going
us the Xmin
values
for
to
these two bounding boxes, which
are represented as X min and X2 min.
(010)
Area of intersection
(0,5)
71, min
x
(min ymin)
max (x, min, x2 min)
We are
going
(xmarymas)
7, max
Xz max
No.
Date
(2 min ymin)
(max, ymax)
minx max, x, max)
Similarly in order to find out the value for Youin and Ymax.
to
compare
The Yimin and Y2 min and Yim
max and max
(90)
Yamin
Jimin
min, ymin)
max(y, min, y2 min)
(xmaxsyman
Similarly in order to find out the valus:
+mas for this blue bounding box, we are going to compare the
values Xmax and x2 max.
z
Similarly, Ymax will be the minimum of 4, max and Y.
Neelgagan
Neelgagan
max
(p)
Samar
y
X
No.
Date
Arie of intusection = (Xmax-x min) *(y.
Aries of Union
No.
Date
max-ymin)
(Aminiymis)
(xmax, ymax)
minc ymayyumar)
(0,0)
min, yemin
XIM
12 max, Yamay
yimax
y
Now once we have these four values which are Xmin /min, Xmax
and Ymax
We com calculate the rea
of
intersection by multiflying the
length and the width of this rectangle, which is the the red
ritangle
(0,0)
x
(min ymix)
(xmaxymax)
Box, Area - (Himax - Fimin) * (y, mar
-gmind
Box Area CX max - X₂ mox) * (y, max -
We are
y max)
Area of Union
Box, Box - Intersection
Now note, when we are calculating the areas of Box! and Box2
actually counting the shaded region twice. So, mis
is the part of both the rectangles. Since the part is counted
twice wi'll have to subtrall it once, in order to get the
ares of union
Haro
Nealgagan
Neelgagan
•
No.
Date
No.
Intersection 2004 Union = (Xmar-*min) & (ymax Jain)
Intersution Over Union =
Evaluation of
Evaluation Metric
Area of intersection
Area of unity
Precision
Date
Let's say, we have a set of bounding box predictions. Alony
with that, we have the IU ocone which we calculated by
comparing these bounding box predictions with the actual bounding
box
and let's say
Predicted
Bounding Box!
we have threshold of 0.5
bucion
10U TP/FP
- mean Average
Evaluation Metrics for Object Detection :-
Interaction of over Union (IoU)
•
9
Meam Auge
Precision. (MAP)
·Mean Average Prevision (MAD
Metric precision, which simply takes into account the
no. of true positives, and is divided by the true positives
and false positives. So, this is basically the actual positive
values upon the predicted positive values.
precision
Z
TP
TP+FP.
2
0.2
FP
"
3
0.9
TP
"
0.0
TP
"
5
0.4
FP
#FP=2
prevision =
3
3+2
=
3
2
0:6
Y
Now ther's another metric which is average prevision.
It calculates the average of the precision values across the data.
Neelgagan
No.
Date
Pradiated
BB
6-7
TP
Average Prevision
IoU TP/EP Precisioni
BB
0-2
Fl
J
66
6.9
TP
0.61
-
BBY
0.8
TP
0.71
3
==
20-61
=0.75
665
0.4
Ff
0.75-
3 = 0-6
T
Total 3.01
APL
2
=
Sprucision;
** (80) x 3.01
0.602
Mean Average Prevision
Meam Average brevision is simply calculated across all the
classes.
YAL
7
EAR
k
21
Modes and Types of object detection.
Types of object detection
Traditional Machine
Learning-based
Approaches ha
Traditional ML-board approaches:
No.
Date
Deep Learning - Based
Approaches
CV techniques are used to look at various features of an image,
such as the color histogram us edges, to identify groups of pixels
that may belong to an object. These features are then fed
into a regression model that predicts the location of the
Object along with its label...
Deep Learning -based approaches
It employ CNND to perform end-to-end rid unsupervis
object detection, in which features.don't need to be defined
and extracted separately
Neelgagan
No.
Date
No.
B
Basic Structure of Object Detection.
Deep Learning based object detection model typically have 2 parts:
•An encoder takes an image as input and runs it through
a series of blocks and layers that learn to extract statistical
features used to locate and label objects.
are then passed to a decodes, which predicts bounding
Outpuls
boxes and labels for each object
• The simplest decoder is a pure regresar. The regresor is
connected to the off of the encoder and predicts the location
and size of each bounding box directly. The output of
the model is the X, Y coordinates pais of for the object
4
•
its extent in the image.
and
Though simple, this type of the model is limited, We need to
specify
the no. of boxes ahead of time. If the images has two
day,
but the model was only designed to detect a
single object, one will
know
unlabelled. However if we
go
we need to predict in each image ahead
of time, pure- ryres es-based model may be a good option.
the no of objects.
An extension of the regreses approach is a region proposal
network.
Date
• In this decodet, the model propose regions of an image where
it believes an object might reside. The pixels belonging the
these regions are then fed. into a classification subnet work to
determine a label Cor reject the proposal).
It then runs the pixal containing those regions through a
Vassification network. The benefit of this method is
more accurate, flexible
model that com propose arbitrary
no of regions Ins may contain a bounding box.
The added accuray, though comes at the cost of
computational efficiency-
Neelyuyan
Neelgagan
No.
Date
No
Object Detection Algorithms:
I Traditional Methods:
invalue
a
combination
Traditional methods for object detection often
of image proussing techniques, feature extraction, and machine
learning algorithms.
1. Sliding Window Method
The sliding window approach for object Detection is a traditional
method when a fixed size window is moved (or "slicks")
across an image to check for objects within each window
(i) Preparation
Before applying the sliding window, we need the following:
.
imag
The image in which
.
Classifier: A pre-
you
rained classifier
want to detect objects.
that com determine whether
object of interest is present in a given window.
This classifier is usually trained on positive and negative
samples of the object.
Window Size: The size of the window that will slide
across the image. This size is typically determined based
on the size of the objects
Neelgagan
we want to detect.
(ii) Sliding the window:
Date
The sliding me window is moved across the image in both
horizontal and vertical directions. The step size or stride,
determines how much the window moves each time. A smally
step size means more windows and a more thorough search
but increases computational cost.
ii) feature Extraction:
for each position of the window, features.
are extracted from the
region of the image covered by the window. These features could be
simple pixel Falues, Histogram of Driented Gradiento CHOG),
or more complex feature descriptors.
(iv) Classification:
The extracted features
aure
fed into the classifier, which determines
Whether the window contains the object of interest or not.
This classifier could be:
• Support vector Machine (SVM): Often used with HOG features
Neural Network: A basic neural network as a pre-trained
deep learning model.
(V) Multi-Scale Detection:
Objects can appear at different ocales, so the sliding window
approach is often applied at multiple scales. The image is
resized (down sampled) several times; and the sliding window
Neelgagan
No.
Date
prowess is repeated on each resized image. This allows the
detection of objects of varying sizes.
(vi) Non- max Suppression (NMS)
After scanning the image multiple scales and locations, we
might have multiple overlapping windows that all detect the
object. Non-maximum suppression is used to combine
single bounding box.
these overlapping detections into a
same
NMS works by
- Sorting
the detected windows by their confidence
→ Selecting the window with the highest.
plous
score and suppressing
all overlapping windows (then with an [ou above a certain threshold
the process for
the remaining windows.
Repeating
(vii Output:
The final output is a set of bounding boxes around detected in ge
objects, along with their confidence scores
#Challenge:
Computational Cost
.
9
Accuray
: The fixed window size
may
not match the
aspect ratio or size of the objects in the image, leading to missed
detections or falar positives
No.
Date
# Improvements:
Several improvements have been made to the basic sliding
window approach to enhance its efficiency and accuracy
• Selective Search: Instead of sliding a fixed-size window, selective
search purposes region candidates based on color, texturs,
Dize and shape
Region Proposal Networks (RPN): Deep Learning methods that propose
regions of interest, reducing the no of windows that need to be
classified.
2. Viola Jones Detector
The viola jones object detection framework is a seminal method
in computer vision, known for its real-time performance in
detecting objects such as faces.
The main steps of the viola - Jones algorithm.
1. Haar like features
2. Integral image
3. AdaBoost Training
4.
Classifier Cascades
are as
follows:
Neelyayan
Neelgagan
No.
Date
No.
Date
I what are Haar-like features?
• In 19th century, Alfred Hams gave the concepts of Hoar wavelets, which
are a sequence of rescated "square-shaped" functions which together
form a wavelet family
or basis
used
•Haan like features are digital image features word in object
All human falls share some universal properties
Mecognition
of the human face like the eyes region is darker than its
region is brighter than the eyle
neighbour pixels, and the nose
region
Oh darker
• A simple way to find out which is region is lighter
is to sumup the pixel values of 60th regions and compare them.
The sum of pixel values in the darker region will be smaller
than the sum of pixels in the lightu region.
be shinier
• If one side is lighter than the other, it may be an edge of
or sometimes the middle portion may
than the surrounding boxes, which cam be interpreted asance.
.
8
eyebrow
They
0
->
are
3
identified
Edge
: types of
Haak-like
features
That Viole and Jone
features, pont live features and four-sided features
boy features and live features are useful for defecting edges
Neelgagan
and lines respectively. The four-sided features are ward for
finding diagonal features
→ The value of the feature is calculated as a single number. The
sum of pixel values in block area minus the sum of pixel
values in white area.
- The value of is zero for a plain surface in which all the pixels
have the same value, and thus, provide nousiful information
+ There are known features that perform very well to detect
human faces:
°
for example, when we
apply this specific haar - like feature
to the bridge of the nose, we get a good response. Similarly,
many of these features to understand if an image
we combine
region contains a human face
What are
integral images?
Inredity, the above calculations. com be very intensiv oince
the no. of pixels would be much greates when we ar
with a large featur
1 dealing
so we can underst
The integual image plays it's part in allowing us to
perform these intensive calculations quickly
- and whither a feature of several features fit the criteria.
Neelgagan
No.
is the name
Date
(also known as oummed- aree table)
•An internal image
a dSA Med to obtain this date
of bon of
is verdas a quick and efficient way
oum of pixel values in an
structure. It is
to calculate the
"rectangular part of an image.
How is Ada Boost ward in viola Jone
• The no. of features
Salgol
imag or
that are present in the 24x24 detector
feature
window is nearly 160,000
· In this algo, each Haar - like feature represents a weak
harmer. To decide the type and Dice of a feature that
goes into the final classifier, words AdaBoast checks the
performance of all cloorifiers. that we supply to it.
• To calculate the performance of a classifices,
it on all subrifions of all images used
we evaluate
training.
• Subregions that will produce a strong response that will
be classified as positives, means it contains human face
And subregions that don't provides strong respons;
don't contain human face.
• The classifiers that performed will are
wught.
Final Result
Stron
On
hosted
Gosfie
No.
Date
gives higher importan
(Contains the best performing weak Jassifice)
when we're training the AdaBoost to identify the imp. features,
we're feeding it info. in the form of training date and subsequently
training it to learn from the info to predict.
Algo is setting
cambe classified
What are
a min. threshold to determine whither something
asa
useful feature or not.
cascading classifiers:
• Maybe the AdaBoost will finally select the best features around
2500, but it is still a time-consuming prous to calculate
these features. for each region.
• We have a 24x24 window which we slide over i/p image, and wenced
to find if any of those regions contain the fore
O
The
Job of cascad is
discard non-faces, and awold
quickly
wooking precioustime and computationtime. They, achieving the
speed necessary for real time face detection.
Neelgagan
Neelgagan
No.
Date
First Stag
-
we have a
omes stages.
classifies which is made up of the best features
next stages,
- In other the nex
we have all the remaining features.
• When an image subregion enters the cascade, it is evaluated
by
first stage. If that stage evaluates the subregion as tue,
meaning that it thinks it's a face, the opp of the stage is maybe...
the
• When a subregion gets maybe, it is sent to the next stage
of the cascad and the process continues as such till we reach the
last stage.
If first ten stage gives a
a - vevaluation, then the imagt
immediately discarded as not as not containing a human
face. Also if it passes first stage and but fails the excond
is
it is discarded aswell.
In Opencv, we have several trained Haas Cascade models
Instead
of creating and
scratch, we use those files
which are saved asxm|files.
training the model fromer
Neelgagan
3. Histogram of Oriented Gradients:
Feature Descriptor
No.
Date
It is a simplified representation of the image that contains only
the most important information about an imag
There are a no
as
0. of feature descriptors, the most popular ones
following:
1. HOG: Histogram of Oriented Gradients.
2. SIFT: Scale Invariant Feature En Transform
3. SURF: Speeded-up Robust featur.
What is Histogram of Oriented Gradients?
are
The histogram of Oriented Gradients (106) is a popular feature
descriptor technique in CV and image proussing. It analyzes
the distribution of edge orientations within an
Object to describe
its shape and appearance. The HOG method involves computing
the gradient magnitude and orientation for each pixel in an
image
and then dividing the image into small ulle.
• Introduction to HOG dioptat feature Descriptas.
+HOG is a feature descriptor that is often used to extract features
from image data. It is widely used for object detection.
Neelgagan
No.
Date
No.
→ The Ron descriptor focuses on the structure is
7. How is it different from the edge features?
shape of an object.
In the case of edge features, we only identify the if the pixel
is edge as not. How is able to provide the edge direction as
well. This is done by extracting the gradient and Orientation
(or we can say magnitude and direction) of the edges.
→ Additionally, these Orientations are calculated in localized
portions. This means that the complete image is broken
T
.
down into smally ryjons and for each region, the gradients
and orientation are calculated.
Finally the HOG would generate a Histogram for each of these
region separately. The histograms are created using the gradients
'Histogram
and orientations of the pixel values, hen & the name
of Quiented credienté.
Basically, the HOG feature descriptor counts the occurrences of
gradients orientation in localized portions of an image.
• Prove of calculating the HOG:
let's take an image of Dize (180x200)
Stept: bruprouss the data (64X1A.)
We need to
Nealyaga
"preprous the image and bring down the width to
Date
height of Mation to 1:2. The image dice should preferably be 64×128.
This is because we will be dividing the imag, into orb and
16* 16) patches to extract fg the features. Having the specified
size (64x122) will makes all our calculations pretty simple.
Step 2: Calculating Gradiento C direction x and y)
The next step is to calculate the gradient for every pixel in the image.
and y directions...
Gradients are the omall
change
inthe
x
Wt us take a small patch from the image having pixel matrix
Con
12/
10
78
96
125
48
152
68
125
145
18
65 09 65
Try
214 56 200 66
214
87 45
102
45
We have shaded os fixed value. Now to determine the gradient
Change) in the x-direction, weneed to subtreet the
vales on the left from the pixel valus on the
right. Similarly, to calculate the gradient in y-direction,
we will subtract the pixel value below from the pixel valus above the
selected pixel
Neelgagan
No.
Date
Henue the resultant gradiento in the x and y direction for this
pixel are
in X direction (4x) = 89-78 = 11
Change in Y direction (hy) = 68-58=8
4
Change
will
give
two new matrices - one storing
This prous
gradients in the x- difuction and over the other storing
gradients in the y- direction. This is similar to using a Sobel
Kernel of size 1. The magnitude, would be higher when ther
is a sharp. Change in intensity, such as around the edges..
The same prous. is repeated for all the pixels in the image.
Steps: Calculate the Magnitude and Orientation
Using the gradients we calculated in the last step, we will now
determine the magnitude and direction for each pixel value.
for this step, we will be using the Pythagoras thcoum.
The gradients
are
hx = 11, Gy = 8.
No.
Date
basically the base and perpendiculas here
Total Gradient magnitude = √(6x)² + (hy)
V
2
J111)² +18)² = 13.6
Calculate the Orientation (or direction) for the same pixel.
Kama yoy you
tamo=
hy
-1
to 02 tam by
3x
The Orientation comes out to be 36 when we plug in the
values. So nows for every pixel values, we have the total gradient
(magnitude)
and the orientation (direction).
We need to generate histograms using these gradients and
Orientations
Neelgagan
Gy
K
4x
Neelgagan
9
No.
Date
Different methods to create histogram using cradients and
Orientation
Ahisto
Method:
Here, we will take each pixel value, find the orientation
of the pixel and update the frequency table.
Here is the frown for meshaded pixel (05). Since, the orientation
for this pixel is 36, we will add ano.
denoting the frequency
against angle valus 34,
Range of Orientation: 0 to 180°
•Binsize : Each bin covers 20°
No. of bin
Z
180
20/bin
29bin
No.
Date
Again for each pixel, we will check the orientation, and store thr
frequency of the orientation values in the form of a 7x1 matrix.
Magnitude
Bin 0
20/40
60
80
100 120,
140 Ho
Frequency
Angle
1751 70180
2
34
35 36 37 38
.
Method 3
:
Magnitude
Bin
13.6
20
40
60
80
100 120/140
The Dame
prous is repeated for all the pixel values, and we end
with a frequency table that denotes angles and the occurence
of these angles in the image.
#Methode
This method is similar to the previous method, exuft mat
here we have a bin size of 20. So, the no.
of buckets we would
Method:
Magnitude
get here is I.
Nealgayan
Neelgagan
M=13.6
0236
(40-56) ₤20 (36-67/20
(1)
1373.6
Bin
0
20
YO 60 80
400
120 140
160
No.
Date
Steps: Calculate histogram of crradients in 8x8 ull (9x1)
The histograms created in the HOG feature descriptor are not
generated for the whole image. Instead, the image is divided
into Ex& ulb, and the histogram of Oriented gradients
is computed for each all.
by dairy so, we get the features. Conhistogram) for the
smalles patches which in turn represent the whole image.
We can certainly change this value here
4
32x32
w
from
8x8 to 16x16.04.
divide the image into 8x8 ulb and generate the histogram
we will get a 9x1 matrix for each all. This matrix is generated
using method 4 Phot
Steps: Normalize gradients in 16x16 cell (36×1)
Why this is done?
Although
we
already
the
imag
are sensiti
have the Hos features created for the 8x3
ull of the image, the gradients of
to the overall lighting. This means that for a particular
picture, som portion of the image would be very bright as
compared to the other des portions
No.
Date
We compot completely eliminate this from the image. But we
cam reduce this light imp variation by normalizing the gradiento
by taking 16x16 blocks.
Her, we will be combining four 8x8 wills to create a 16x/6 block.
So, we will have four 9x1 motrices on a single 36x/matrix
Tonormalize this motrin, we will dwide each of these
values by
the square root of the sum of squares of the values
Xi [al, azad,... ass]
V: [a,, 92, αz,...., as]
2
Vectors
M = √(@₁)² +(a2)² + (os)² + ..... + (034)²
And divide all the value in the vector V with this valusk:
Normalized vector =
α
+
k
K
Steps features for the complete image
An, we have berated features for 16x16 blocks of the image. Now,
we will combine all these to get the features for the final
image
Neelgugan
Neelgagan
No.
Date
We would have 105 (7x15) blocks of 16x16. Each of these
blocks has a vector of 3+x) as
features.
Hence, the total features for the image would be s
105x36x1 = 3780 features
II Deep Learning Methodo
• Region Based Methods.
are a
riione
in an
image
that
are
No.
Date
105
networks and region -based approaches
Y
3. Compute
CNN featur
4. Classify.
Regione
The region -based methods for object detection.
family of red techniques that invalus identifying
likely to contain objects and
then classifying these regions into different Object categories.
These methods generally involve two main steps.
• region proposal and region dassification
*. region classification
K.
1.R-CNN
• Region-based CNN
vision tasks.
is atype of deep learning architecture and for
computer
need for abject detection in
• RCNN was one of the pioneering models that helped advance the
Object detection field by combining the power of convolutional neur
Neelgagan
1.5/pimage 2. Extract region
proposals
How R-CNN works? 1. Region Proposal:
in the image that are
• RCNN starts by dividing the imput images, into multiple regione
Or subregions. These region are
Or "region candidates". The region proposal steps is responsibl
referred to as "region proposals "
for generating a set of potential regions in the
likely to contain objects.
R-CNN does not generate these proposals itself; instead it relies on
external methods like Sulective Search arEdge Boxes to generate region
proposals
05
• Selective search, for example, operates by merging and splitting
segments of the image based on various imag, was like color,
texture and shape to create a diverse set of region proposals
2. Feature Extraction:
Once the region proposals
regions
are extracted and
are generated, approximately 2000
Warped
to a consistent input size that
the CNN expecto ( eg. 224x224 pixels) and then it is passed through the
Neelgagan
No.
Date
No.
Date
4
CNN to extract features.
The CNN
• Before warping, the region size is expanded to a new size that
will result in 16 pixels of context in warfed fram.
used is Alex Net and it is typically fine tuned on a
like Image Net for generic features representation.
lagedatant
The output of the CNN is a high-dimensional feature vector referesenting
the content of the region proposal.
3. Object Classification
The extracted feature vectors from the region proposals are fed into
separate ML classifies for each Object class of interest. R-CNN
typically mees SVM for classification. For each dass a unique
SVM is trained to determine whether or not the region proposal contains
an inot any of that class.
During training positive samples are regions that contains un
instance of the class, -ve pamples are regions that do not.
4. Bounding Box Regression.
R-CNN also performs bounding box regression, for each class,
a separate regression model is trained to refine the location and size
of the bounding box around the detected object. The bounding box regression.
Nenigagan
helps improw the accuracy of object localication by adjusting
initially proposed bounding box to better fit the object's actual
boundaries.
5. Non- Maximum Suppression (NMS)
ryion
the
After classifying and regressing bounding boxes for each
proposal, R-CNN applies non-maximum suppression to eliminate
duplicate or highly overlapping bounding boxes. NMS ensures
that only the most confident and non-overlapping bounding boxes
are retained as final objects detections
Steps involved in NMS:
1. Score thresholding
2.
Segin by discording all bounding boxes with confidence scores
(predicted probabilities) below a cutain threshold. This step ensures that
Only boxes likely to contain objectoare considered
Surting by Confidence:
Sort the remaining bounding boxes based on their confidence
scores in descending order. The box with the highest confidenc
score is considered the most likely to contain an object.
Neelgagan
No.
Date
No.
Date
.
4.
3. Selection and suppression
• empty
find
alted bb
list to hold the
in the sorted list:
• Initialize an
.
While there are o fill boxes
mth
→Select the box with the highest confident score (the first box.
list after sorting) and add it to the final list of sluts boxes
→ Compute the Loud b/w the box and all other boxes in list
→ Remow allboxes that have ants IOU greater than a predefined
threshold with the selected sox.
The Lou threshold is a parameter that can be tuned; common
values are b/w 0.3 and 0.5. This step ensures that boxes that
nignificantly overlap with the selected box. are suppressed.
Repeat
Refeat the prows with the remaining boxes, selecting the next highest
confidence box, adding it to the final list, and suppressing overlapping
boxes
unhl nomore boxes remain
Strengths of RWN:
•Accurate Object detection
Robustnes of Object variations.
.
flexibility
Disadvantages of RCNN
• Computational complexity
slow inference
overlapping region proposals
R-CNN is not end to end
Fast RCNN
Fast RCNN produus region proposals based on the last featur
map of CNN, not from the original iffimage like R-CNN, In other
wondo, foot RCNN implemento CNN first and then generates.
region proposals. On the contrary, R-CNN produces region
proposals first and then implements CNN.
Proposed Rots
have diff.
Dizeo
Bounding box
Regressor
JROZ pooling tages
Softmax
dassifie
Fixed Ko Jo after
Fin
the Not podiy
Lage
□ KuZextractor
ComNet
i/p image
feature
extractor
Coelich veSearch
Jaws of
layers
Neelgagan
.
No.
Date
The architecture of Fast R-CNN consists of the following modules:
4
feature extractos modul: The metual starts with CNN to extract
features from the full image
ROI Extractor: The selection search algorition propores region.
Candidates per image
component
that was
introduced
• ROI pooling Layer: This is a new
to extract a fixed-size window from the feature map before
the ROIs to the fully connected layers. It uses max
sending
fooling to convert the features inside any valid ROI into a
small feature map with a fixed spatial extent of height x width
(HXW). In short, the pooling layer is applied on the last feature
map layer extracted from the CNN, and its goal
fixed-size Rote to feed to the fully connected layers and then
the output layers.
⋅
is to extract
Two -head of players: The model branches into two heads. A
Dettman classifier layer that output a discret for probabilitys
distribution per ROI. Abounding box regressor layer to predict
offade relative to the original hot
Neelgagan
FX
Faster RCNN
No.
Date
Similar to Fast R-CNN, the image is provided as am input to a
convolutional network that provides a convolutional feature map.
Instead of using a selective search algorituion on the feature map to
identify the region proposals, a region proposal network (RPN)
is used the predict the region proposals as part of the training
prous.
The input be image is presented to the network, and its
pretrained CNN. These features,
features.
are extracted via a
in parallel
The architecture of fastes RCNN com be described using two main
networks
• Region Proposal Netwash (RPN):
Selective search is repland by a CNN that proposes Ro1o from the
last feature maps of the feature extractor to be considered for
investigation. RPN is faster than the selective search algorithm
because RPN is based on GPU while the selective search algo is
based on CPU. The RPN has ture outputs. The objectness
and the box location.
scose
Note: At this point we don't know what the object is, just
that there is potentially an object at a certain location in the image.
Neelgagan
No.
Date
No.
Date
fast R-CNN
It consists of the typical components of foot RCNN.
base network far the feature extractor: a typical pretrained CNN
model to extract features from the input image. RoIpooling
opp layer that contains tur fully
layer to extract fixed-size
connected layers. A softmax classifics to outfout the class probability.
and a bounding box regression CNN for the bounding box predictions.
The off is then passed into 2 fully connected layers.
This architecture achieves an end-to-end trainable, complete
Object detection pipeline where all of the required components
are impide the hetuak:
Base network extractes, Region Proposal, Rot pooling,
Object Classification,
and Bl-rigresses.
features and limitations
R-CNN
9
selutive search to
extractRo In (~2000)
A ConvNetinued to
Foot RCNN
→
fastes RCNN
regression
• passed only once to the CNN retos Proposal network,
and feature maps are extracted which makes algo much fastes
A ConNet is used to
extract features from extract feature maps from
~2000 regions extracted the lift image. Then region.
sproposals are generated using
-
An end-to-end DL
retwe
Slow computation
selective search.
.
Selective search is slow.
object proposal tak
time
timis still low
as each region is and hence, computation
passed to the CNN
separately (~2000)
Mask RCNN
Network, is
Mask RCNN, short for Mask Region -based Convolutional Neural
an extension of the faster R-CNN Dbject detection
algorithon ward for both object detection and instance segmentation
tasks in computer vision.
The significant changes introduced by Mask RCNN from faster RCNN
1.
Core
Replacing ROI Pool with ROI Align to handle the problem of
mis alignments b/w the input feature map.
2. The ROI foaling grid
3. The war of Feature Pyrannid Network (FPN) that enhances the
capabilities of Mad RCNN by providing a multi-siale feature
representation, enabling efficient feature reuse, and handling
scale variations in objects.
Neelgagan
No.
D
Date
Note: as what sets Mask R-CNN apart is its ability to not only detect
objects within an image but also to precisely segment and identify.
the pixel - wise boundaries of each object. This fine-grained segmentation
capability is accomplished through the addition of an extra "mock head"
faster
branch to the
RCNN modl
What is Mask R-CNN?
• It is a deep learning model that combines object detection
and instamu segmentation. It is an extension of the Faster
.
R-CNN architecture
an extra "mask head"
• The key innovation of Mask RCNN lies in its ability to perform
pixel-wise instance symentation alongside Object detection.
This is achieved through the addition of
branch, which generates precise orgmentation masks for each
detected objects.
Two critical enhancemento integrated into Mack RCNN are
ROI Align and feature lyramid Network (FPN).
Noeliquyan
→
Roz Align addresses the limitations of the traditional
ROT foaling methed by using bilinear interpolation during
the posting process. This mitigates misalignment
ensures accurate spatial information
capture from the input feature map, leading to improved
joontes issues and
segmentation accuracy, particularly for small objects.
No.
Date
+ FPN plays a pivotal role in feature extraction by constructing
a multi-scale feature pyramid. This pyramid incorporate
features from different scales, allowing the model to gain
a more comprehensive understanding of object context.
and facilitating better object detection and segmentation.
across a wide range of object sizes.
*Mask RCNN Architecture
built upon the faster RCNN network
an extra "mask head" branch of pixel-wise
the
with
addition
of
segmentation.
pre-trained
I Backbone Network!
The backbone network in Mask RCNN is typically
Convolutional neural network, such as ResNet Or ResNet xt.
This backbone processes the i/p image
.
-
features.
and extracts high-leve
An FPN is then added on top of this backbone network to
create a feature pyramid.
FPNs are designed to address the challenge of handling objects
varying sizes and scales in
an
image. The FPN
architect use crates a multi-scale feature pyramid by combining
features from different level, from high-resolution for
features with rich semantic info.
Neelgagan
to low-resolution
No.
Date
The FPN in Mask RCNN consists of the following steps:
1. Feature extraction: The backbone networks extracts high-
features from the i/p image
features
-level
at
2. feature fusion: FPN creates connections between different levels of
the backb one network to create a top-down pathway. This top-down
pathway combines high-level semantic information with lower
level feature maps, allowing the model to reuse
different scales.
3. Feature Pyramid: The fusion proces generates a multi-scale
feature pyramid, where each level of the pyramid corresponds.
different resolution of features. The top-level of the pyramid
contains the highest resolution features, while the bottom level
Contains the lowest-resolution features
to
II Region Proposal Network (RPN)
The RPN is responsible for generating region proposals or
candidate bounding boxes that might contain objects within the
image. It operates on the feature map produced by the backbone
network and proposes potential regions of interest
-
Neelgagan
No.
Date
objedn
Poigmas
, b, 250 Recu.
Trishafee
(N, wxhx3)
(no. of aspect ratio = 3)
N: batch pize
(€ 4,8,163264
for each pyramid of feature
(12-3(aspect ratio/anchoropes loc) *4 (could))
No.
Date
• The primary purpose of Ro1 Align is to align the features within
a region of interest (ROI) with the spatial guld of the o/P feature
map. This alignment is crucial to prevent info. lass
that com occur when quantizing the ROI's spatial coordinates
to the nearest integer.
1x W124
Rushake
+34)
IL ROI Align
RPN
RPN bbox
predictions
After the RPN generates region proposals, the ROI Align layer
is introduct. This steps help to overcome the misalignment
issue, in ROI Pooling
Newliquya
The ROI Align process involves
(1) Imput feature Map: The process begins with the i/p feature
map, which is typically obtained from the backs on
network. This feature map contains high level semantic
info. about the entire imoge
ii Region Proposal: The RPN generates region proposals
(candidate bbox) that might contain objects of interest
within the image
Neelgagan
No.
G
fixed
Date
\iil Dividing into (veids: Each region proposal is divided into
number of equal-sized spatial bins or grids.
These grids are used to extract features from the imput.
feature map corresponding to the region of interest"
(iv) Bilinear interpolation: unline ROI pooling, which quantizes.
The spatial coordinates of the grids to the nearest integer,
Ro1
uses bilinear ordinate
interpolation
to
Align
calculate the pooling contributions for each grid. The
interpolation ensures a more precis alignment of the
features within The Roz.
(V) Outfout features the features obtained from the input
feature map, aligned with each guid in the output
flatur map,
are used as the representative features
for each region proposal. These aligned features captures
fine-grained spatial information, which is crucial
for accurate segmentation.
.
Mash Head
Additional branch in Mash RCNN, responsible for generating
segmentation masks for lach region proposal
· The head uses the aligned features obtained through ROI Align
to predict a binary mash for each object, delineating
the pixel-wise boundaries of the instances.
The Mask Head is
No.
Date
typically composed of several convolutional
layers followed by upsample layers ( decon valutional or
transposed convolution layere)
During training the model is jointly optimized using
a combination of classification loss, bounding box
los and segmentation loss.
This allows the model to
regression
learn to simultaneously detect Objects, refine thuis bounding
boxes, and produce previse segmentation masks.
# Limitations
Computational complexity
Small- Object segmentation
4 may struggle to accurately syment vergomall
Objects due to limited pixel information
• Data Requirements 3
↳ Training Mack RCNN effectively requires a
large amount of ammotated date, which can be
time-consuming and expensive to acquire
limited Generalization to unsen Categorice
Object
4 model's ability to generalize to whern
Categories is limites, especially when data is scarce
Noelqagen
Neelgagan
No.
Date
No.
Date
.
.
.
• Single Shot Detector
# What is a Single Shot Detectos (SSD)?
SSD is
an innovative object detection algorithm.
It stands out for its ability to swiftly and accurately detect
and locate objects within images or door video frames
a single pass of
It has the capacity to allcomplish this in a
a deep newral network, making it exceptionally efficient
and ideal for real-time application
· SSD achieves this by employing anchasboxes of various
aspect ratios at multiple locations in feature maps.
These anchor boxes enable it to handle objects of
different sizes and shapes effectively. Morever, SSD
use multi-scale feature maps to direct objects at various
scales, ensuring that both small and large Objects in the
image are accurately identified.
B
It balamus b/w speed and accuracy
It is known for its ability to perform object detection
in real time and has been widely adopted in
Various applications, including autonomous
surveillance and Augmented Reality
Neelgagan
driving,
# Key features of SSD
• Single Shot: SSD performs object detection in single pass
through the network. It directly predicts the presence
of objects and their bbox coordinatics in a single shot,
making it faster and more efficient
•MulhBox SSD uses a
set of default bounding boxes (anchor
boxes) of different scales and aspect ratios at multiple
locations in the input image. These default boxes serve
as prior knowledge about where objects are likely to
appear. SSO predicts adjustments to these default
boxes to locate objects accurately
Class Sures: SSD not only predicts the bbox coordinates but
also assigns class scores to each default box, indicating the
likelibat of object belonging to a specific category.
am
(e.g. Can, pedestrian, bicycle).
· Hard Negative Mining: During training, SSD employs harmful
mining to fours on challenging examples improving the
model's accuracy.
#Key Concepts of SSD
·
Default
predefined
Bounding Boxes (Anchor Boxes): SSD uses a
set of default bounding boxes, also known as anchor boxes
Neelgagan
No.
Date
These boxes come in various scales and aspect ratios, providing
prior knowledge about where objects are likely to be located
in the image. SSO predicts adjustments to these default
boxes to localize objects accurately
•Multi-Scale Feature Maps: SSD operates on multiple featur
maps at different resolutions. Obtain these feature maps
layers to the input imagt at various
by applyinsing
stages. Using feature maps at numerous scales allows SSD
to direct object of different sizes.
Convolutional
• Multi-Scale Predictions: for each default bounding box,
SS.D makes predictions at multiple feature map layers with
different resolutions. This enables the model to capture
Objects at various scales. These predictions include class
scores for different object categories and offorts for adjusting
the default boxes to match the object's positions.
.
• Aspect Ratio Handling: SSD uses separate predictors (convolut
-ional fitters) for different aspect ratios of bounding boxes.
This allows it to adapt to objects with varying shapes. and
copect ratios.
# Architecture of SSD
-a
No.
Date
for
real-time
dup convolutional neural network (CNN)
object detection. It combines various layers to perfe
localization (bounding box prediction) and classification
(object codegorry detection) in a single forward pass.
- robust Object detection framework based on a feed-forward
CNN
SSD Approach
.
Base Network (Truncated for classification):
SSD begins with a standard CNN architecture, which is
typically used for high-quality image classification tasks.
However, in SSD, this base network is truncated before any
classification layers. The base network is responsible for
extracting essential features from the impit image.
Multi-Scale feature
Maps: Additional Convolutional layers ay
added to the truncated baar network. These layers progressives
reduce the spatial dimensions while increasing the no. of
channels ( feature channels). This design allows $550 to
produce feature maps at multiple scales. Each scale's feature
map is suitable for detecting objects of different sizes.
Neelyagua
Neelgagan
No.
Date
+ Default Bounding Boxes (Anchor Boxes): SSD associates a predefined
out of default bounding boxes with each feature map ull.
These default boxes have various scales and aspect ratios.
The placement of default boxes relative to this corresponding
ull is fixed and follows a convolutional grid pattern.
for each feature map cell, SSD predicts the offsets .necessary
to adjust these default boxes to fit objects and the class scores
indicating the presence of specific object categories.
+ Aspect Ratios and Multiple feature Maps: SSD employs default
boxes with different aspect ratios and uses them across
multiple featured maps at various resolutions. This approach
efficiently captures a range of possible object shapes and
, on an intermediate fully connected
SSD' doesn't rely o
layer for predictions but uses convolutional filters directly
Dize
# Training of SSD Method
Assignment of Ground truth information: In SSD, we assign
ground truith information i.c., actual object locations and
categories, to specific outputs within the fixed set of
detector outputs. This prous is crucial for training the
model to recognize objects correctly.
Matching Strategy: During training, SSD matches lach ground
truth box to the defaull boxes based on the best Jaccord.
aurlap- Jaccard overlap mecours how much the predicted
Neelgayan
No.
Date
box overlaps with the
ground truth box
Any default box with a Jaccard ourlap higher than a threshold
• (typically 0.5) with a ground truth box is considend a
match. Unt
Unlike other methods, such as Multibox, SSD allows multiple
default boxes to be matched with a single ground truth box
if theirs Jaccard overlap exceeds the threshold. This simplifics
the learning problem, as the model can predict high
Broses for multiple overlapping default boxes instead of being
forced to select Only one.
Training Objective:
1000
The training objective for SSD is derived from the MultiBox
objective
but is extended to handle multiple Object categories.
It includes both localization and floss and a confidenc
loss (comp).
Detector Outputs
In this ground truth info. must be assigned to specific
outputs in the fixed set of detector outputs.
Localized Loss (Roc): This loss measures the difference
between the predicted box parameters (eg. box coordinates) and
the ground truth box parameters. It was a Smooth US
loss function
Neelgagan
→
No.
Date
the
Confidence lxo (L comp) Calculate this type of loss for
using a softmax loss over multiple classes. It measures
difference between predicted class scores and the actual class
labels. The overall loss function is a weighted sum of
confidence lowes.
the localization and
+ Hard Negative Mining: To address the imbalanu between
the (matched) and -ve (unmatched examples during training,
It selects a subset of
SSD uses hard harmful mining
-ve examples based on the highest confidence loss for each
default box. The goal is to maintain a reasonable.
-ves and & ves. (around J'D for more
Matio between:
efficient and stable training
→ Dato Augmentation: Apply date augmentation to make
the model robust to various input object sizes and
shapes. During training, lall input image Lambe
subject to multiple transformations, including cropping
resizing and horizontal flipping. These augmentations
helf the model generelize bettes to real-world scenarios.
Neelgagan
No.
Date
* Image pre-proussing techniques in deep learning and AI
1. Resizing
2 Normalization
3. Data Augmentation
M. Mean Subtraction and Standardization
S. Grayscale Conversion
6 Histogram Equalization
1. Denoising
& Edge Detection
9 Image Binarization
10. Color Space Conversion
I. Channel - wise Normalization
17. Cropping and Padding
* Video pre-proussing techniques in deep learning and AI
1. Frame Extraction
2. Frame Rate Adjustment
3. Resolution Adjustment
M. Temporal Smoothing
5. Background subtraction
6. Optical flow Calculation
4↳ estimating the motion b/w consecutive frames to
capture dynamic information
7. Temporal subsampling
8. Color Correction
Neelgagan
No.
Date
No.
9. Codec Conversion.
10 Stabilization
4 correcting for unwanted camera movement to
stabilize the video footage
11. Segmentation masks Cremuration:
4 Pre-computing segmentation masks to aid in tasks like
object detection and tracking.
12. Data Augmentation for videos
Applying transformation like random cropping
flipping, rotation, and temporal jittering
to increase the diversity of the training
# Examples of tools and libraves for pre-prousing in DLand AI
.
Tensorflow and Keras
.
4
Py Torch
openc
Sukit-image
albumentations
imgaug
b
9
Neelgugun
# Image foot-processing techniques in DL and AI
1.
Thresholding and Binarization
2. Non-Max Suppression (NMS)
3. Contous detection and Analysis
4. Morphological operations
Date
4 like dilation, erosion, opening and closing to
refine segmentation mask.
5. Heatmal Generation
6. Image Reconstruction
4
Enhancing and reconstructing images from low-quality
inputs, often using generative models like GAND.
7. Super-Resolution
8. Deblursing and Demoving
D&. Saliency Mapping
4 Generating maps that highlight the most important regions in
an image for model predictions.
* Video post-processing techniques ind Land AI
1. frame interpolation and super Resolution
2. Video Stabilization
3. Object tracking
4. Temporal smoothing
5. Action Recognition and Annotation
6. Background Subtraction and Foreground segmentation
Neelgagan
No.
Date
(93,67) (107,123)
(32,57) 223,178
No.
Date
7. Optical Flow Estimation
9. Stene Change Detection
9. Video Summarization
10. Visual Effects and Enhancements
* Example of toals and libraries for post processing and pre-processing
in D'Land AI
.
•Tensorflow and Keras
.
PyTorch
"
OpenCV
⋅
.
scikit-image
ffmpeg
Detefron
Open los
# Image Data Augmentations techniques
Rotation
X. Flipping
3 cropping
Y. Scaling
Translation
b. Shearing
2. Brightness Adjustment
8. Contrast Adjustment
& Saturation Adjustment
10. Hue Adjustment
11. Gaussian Noise
12. Blur
13 Elastic Deformation
Do th. Culout /Random Exasing
DT·
15. Mixup.
# Video Data Augmentation technique
1. frame dropping
2. Temporal shifting
3. Speed variation
4. Temporal bropping
5. fram flipping
6. Rotution
1. Translation
Neelgagan
Neelgagan
8. Scaling
9. Brightness, Contrast, Saturation
10. Gaussian Noise and Blus
11. Elastic Deformation
12. Cutout/ Random Emasing
13. Mixup for frames.
Adjustments
Noelgagan
PyImage Search
No.
Date
# Image Classification pipeline.
Step). Gather your dataset
Step 2: Split your dataset
4 [train, test]
Step 3 Trom your network/mochl
Step 4: Evaluate the model
# Parameterized Learning - Abetter Approach
No.
Date
1. Define model in terms of a smaller no. of parameters.
2. Learn patterns from data during trainingtime
3. Spend more time upfront during training
4. Model will always be same size.
regardles of dataset size
# Four components of paramet wized learning
1. Data: Raw pixel intensities/ extracted features and class labels
Scoring function: INPUT_IMAGES. ? FC INPUT IMAGES)=
OUTPUT CLASS LABELS
2.
3. Loss function: Measur agreement between predicted class
labels and ground-truth class labels.
4. Weights and biases: WL weights) and bl biases) - will be updated
Whoining during training prous.
Neelgagan
No.
Date
No.
Date
# Definition
• InputData x; and yi
-
izly
2
N and y;= 1, k
N total date points.
Each data point is D-dimensional
K total Catyories (ie, unique class label.
flx; W, b) = Wx; +b
Dx, KD
Kv
- D is the dimensionality of the image /featurevector
-Kis the total number
of class losets
# Los function and Optimization method
Loss function
"Los function tell us how good
Corbad) we're doing at making
Corrut predictions.
Neelyagan
Optimization method
Optimization methods look
at our loss and then update
thewrights of the ncton hetuates
such that we may make better
predicion
Binary Cross Entropy
Categorical bros Entropy
Mean squared Ersa.
# Gradient Descent Algoritions.
->
Gradient Descent Analogy.
Imagine
we are
Su
RMS Pup
Adam etc.
and Variation
nining blindfolded in the mountain
+ No GPS, but we have this little box in our back pocket which
we call our " weight matrix "
->
This matrix monitors our steps, like an accelerator, and
gives us
This
suggestions on when to go
based on our movement.
weight matrix also updates itself based on our movements
+ Our goal is to get to the bottom of the Maving whe's we
find a rives, get
com
help
What is Gradient Descent
some water,
and eventually find
I. We start by taking our cost / loss function ( ice. The function
responsible for computing the value we want to minimize.)
2. We then compute the gradient of the loss.
3. And finally,
we take a step in the direction opposite of
since this will take us down the path to our local
The
Neelgagan
gradient
minimum)
No.
Date
# How does Gradient Descent power neural networks and
dup Learning?.
1. Ve initialize the neural networks with a random set of
weights
2. We ask the neural network to make a prediction
point from the training set
on a data
3. We compute the prediction and then the loss/ cost function,
which tells us how good/bad of a job
the correct prediction.
4. We compute the gradient off the loss.
we did at making
5. And then we eves -so-slightly tweak the parameters of
The neural network such that our
& Vaniations:
•Vanilla GD
-Stochastic GD
B
·
Min-batch SGD
(SGD)
ShD with momentum.
predictions are better.
SGD with Nesterovaculuation
I Vanilla Gradient Descent
Consider an image dataset of N = 10,000 images. Our goal is to
train a neural network to classify each of these 10,000 images
into a total of 7 = 10 categorica
Neelgagan
1. We run all 10,000 images through our network
2. We compute the loss and gradient
3. We update the parameters of the network
LE
No.
Date
In vanille gradient descent we only update the network's weights
once per iteration, meaning that the network sees the entire
awright update is performed:
dataset
every
FT •• Problems:
a
hime
• If the no. of training examples is large, then vanilla gradient
decent is going to take a long time to conveys der to the fact that
weight update is only happening once per data cycle
Furthermore, the larges your dataset goals gets, the more nuanced
the gradients cam become, and if we're only updating the weights
once per epack then we're going to be spending the majority
the cases time computing predictions and not much time
actually barning (which is goal of an oftimication problem)
II Stochastic Gradient Descent (SGD)
Unlike Vanilla gradient descent, which only does one wright update
per epoch, Sal insted does multiple weight updates.
Algarithm:
1. Until Convergence
Neelgagan
1. Rand only select a data point from the dataut
No.
Date
No.
Date
2. Mohu a prediction on it
3. Compute the love and the gradient
4. update the parameters of the network
SGD tendo to
conveye
mure faster because it's able to start improvize.
itself after each and every weight update.
II Mini Batch Sup
·
• While SGD cam convuge faster for large datasets, we actually res
.
into another problem.
- we cannot lever age our vectorized
libraries that make training super fast.
There is a variation of SGD called mini - batch SGD that
solves this problem
When we hear people talking about ShD what they
always referring/
to is mine batch shD.
al
Mini-batch SGD introduces the concept of a batch size, S. Now
given a dataset of size N, there will be a total N/S updates to
the network
1. Randomly shuffle the input date.
2 Until convergence.
1. Select the next batch of date of sizes.
2. Make predictions on the subset
Neelgagan
3. Calculate the loss and mean gradient of the mins - batch
4. Update the parameters of the network
Neelgagan
No.
Data
#
Steps to create an
image classification model with
tensorflow
1. Setups and install dependencies
D
0 + Install necessary libraries: Tensorflow, Numpy, Motplotlib.
2. Impart libraries
3. Load and Prefrous the dataset.
Load the dataset
Normalize the
images
toa
ohol
Mange
+ split the dataset into training and testing
4. Define
the Model Architecture
+ choose a suitable model
->
(e.g. CNN, ResNet)
sets.
architecture
Define the layers of the model using Tensorflow/keras
5. Compile the Model:
-
Specify the optimizer, loss function, and evaluation matrix...
6. Train the model
→ Fit the model on the training data
-
Specify the number of epochs
7. Evaluate the Model:
and batch size
→ Assess the model's performance on the test data.
accuracy
and other relevant metrics.
Calculate
Nexliquan
No.
Date
8. Visualize Training History (Optional)
→ Plot training and validation accuracy / loss over epochs.
9. Make Predictions (Optional)
→ Use the trained model to make predictions on new data.
10. Save and Load the Model (optional)
Save the trained model to disk
+ Load the saved mode and future use
#Detailed Steps
Step 2: Import Libraries
import tensorflow as #f
from tensorflow. kuras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
Step3: Load and Preprocess the dataset
#Load CIFAR-10 dataset
C train-images, train labels), (test images, test labels) =
Gatasets. Cifar 10. load-data (
# Normalize pixel values to be between Oand/
train images, test images = train-images/255.0,
"test-images/25°5.0"
Neelgagan
No.
Date
No.
Date
Stepy: Define the Model Architecture:
model = models. Sequential ()
model, add (layers. Conv2D (32, (3,3), activation = 'rel',
input shape=(32, 32,3)))
model, add Clayers. MaxPooling 2D(2,2))
model, add (layers. Conv2D (64, (3,3), activation = relu))
model, add ( layers. Mox Pooling 20 ((2,2)))
model.add(layers. Conv2D (64, (3,3), activation = 'relu)
model. add layers. Flatten ())
model, add (layers. Dense (64, activation = 'reclu'))
model. add Clayers. Dense (101)
Steps: Compile the model:
model.compile (optimizer = 'adam'; loss = tf. keres. losses.
Sparse Categorical CrossEntrify
(from logits ? Trus), metrics = ["accuracy"])
Step 6: Train the Model:
history = model. fit (train images, train labels, epochs = 10,
validation-data = (test-images, test-labels))
Neelgagan
Step 1: Evaluate the Model
test loss, test-acc = model. evaluate (test-images, test labels,
sole Verbose=2)
freint (f" In Test
"In Test Accurely
: { test acc 3')
Steps: Visualize Training History (Optional)
felt. plot Chistory. history ["accuracy" ], label = "acumayy)
" plt. plot ( history, history [ "val_accuracy" ], label = "Val-ouery()
"felt. x label ('Epoch')
Jelt. ylabel ('Accuracy')
felt, ylim (10,17)
falto legend (loc = ' lower
plt. show's
right')
Step 9: Make Predictions (Optional)
predictions = modil. predict (test-images)
# To get the class with the highest probability.
predicted classes = np.orgmax (predictions, axis=1)
Step 10: Save and load the model (Optional)
save
"model, saw ("my_model. hs")
new model = ff, keras. models, load-model ((my-model, hs1)
Neelgagan
No.
Date
No.
#ResNetso
In: resnet = ResNet so (input_shape = IMAGE-SIZE +131,
weights = "image net", include_top = False )
A
input-shape IMAGE-SIZE.. +[3]: specifies the
shape of the input images. 'IMAGE SIZE is a tuple
representing the dimensions of the images (e.g. '(224,214)
Adding '[3]' indicates that the
(RGB)
images
have 3 Channel
• "Weights = imag Net': loads the pre-trained weights
of the model trained on the Image Not Dataset
•'include_top= False: Excludes the fully connected (dened layers
at the top of the network. This is useful when we want
• feature extractor and add your
to use the pre-trained model as a
Our custom clasification layers on top.
* Freezing the layers:
In: for layer in resnet. layers:
layer. trainable = False
This loof iterates ours all layers of the 'resnet' model and sets layer.
trainable fabe. This means that layers weights will not be
Neelgagan
Date
updated during training. Freezing the layers is a common practice
when using a pre-trained model to avoid modifying the pre-traine
weights, axetly especially when we want to use the model for
feather extraction.
# x = Flatten () (rusnet. Output)
1. 'resnet. outful':
+ This refers to the output tenson of the pre-trained ResNet so
model. Sinu the include top = false parameter was set
when creating the ResNet 50 model, the output tensor
will be a 4D Pensor (batch_size, height, width, channel) resulting
from the final convolutional layer of ResNetro, excluding
the fully connected (dense layers at that are typically
Meed for classification.
2. Flatten !
t
It reshapes the 4.1) tensor (batch size, height, width, channels)
into a 20 tenses (batch size, features). The 'features' dimension
is the product of the height, width and channels dimensions.
1. 'Flatten U (resnet. Output):
ResNetso
This applies the 'flatten' layer to the output of the
model. The result is that the multidimensional feature maps
produced by the convolutional layers of Res Net 50 are transforms
Neelgagan
No.
Date
into a single vector for each image in the batch.
→ This transformation is necessary before feeding the features.
into dense (fully connected) layers, which requires
a2D tensor input (batch size, features)
fosex
If the output shape of resnet is (batch_size, 7, 7, 2048)
"it means there are 2048 feature maps of size 7x7 for each
image in the batch.
After flattening
represented
The shape becomes (batch size, 7*7*2043) 1.c. (batch-size,
100 352). Each image in the batch is now.
by a single 100352 - dimensional vector.
11
In prediction Dense (3, activation = 'softmax')(x)
mode: Model ( inputs = resnet inputs outputs prediction)
•Dense (3, activation 'softmax) (x)':
This creates a fully connected (dense) by layers with 3
output, units and a softmax activation functions:
+ "`3: The no. of output units corresponds to the no. of classes
in the classification problem. In this case, there are 3 classes.
Noelgayan
No.
Date
+ activation: "softmax : The softmax activation function.
converts the raw output scores from the dense layes into
probabilities, where the sum of the probabilities across the 3
output units is). This is commonly used for multi-class
classification problems...
Exy : This applies the dense layer to the output of the previous
layer ('x')"! Here 'x' is the output from the 'flatten" lays or
any
Other preciding layers added to the mode.
# Creating the model
Y
In model = Model (inputs = resnety-input, outputs = prediction)
Model (input = resnet. imput, output = prediction)
This creates a new model instance in Kesas
:
+ "inputs" resnet. input" : Specifies that the impurt to this
model is the same as the input to the pre-trained ResNetso
model. This means the model will accept input image
of the same shape as defined in resnet, input.
'output = prediction' : Specifies that the output of this model
is the prediction layer, which is the dense layer we just
added with 3. units and softmax activation.
Neelgagan
No.
Date
# Semantic Segmentation
Date
9
Use la
• Multi-Can classification:
The model is designed for a multi-class classification
usis softmax activation
task with 3 classes. The output layer
to produce a probability distribution over the 3 classes for
lach imput image.
Key points to the image classification with Res Netso
feature Extraction with Res Netso model is used to extrad
features from the imput images
2. Adding Custom Layers: Custom layer are added on top of Res Nets o
to adapt the mode to the specific classification task. In this
cace, a dense layer with ice units and a final output layer.
with 3 units and softmax activation are
added.
3- Model compilation The model is compiled with the Adam
optimizer, categorical cross- entropy los and accuracy metric.
4. Model Summary: The "model. summary ()" funtion
provide a summary of the model architecture including the
layers, outhout shapes, and the no. of parameters
field of
amalya's
+ Semantic segmentation is an important task in the
computer vision and getting a lot of attention due to deep learning
techniques providing high level of accuracy for image on
Image Segmentation refers to the partitioning of am image
into distinct region or categories, with each region
containing pixels with similar attributes and each pixel in
image being allocated to one of these categories.
->
83
The
goal of semantic segmentation is to train a model which
can look of the image of multiple objects and infes semantic
information from the images to detect and recognize individu
Objects present in the image
→ Semantic image segmentation is a computer vision technique
used to understand what is in a given image at a pixel level.
lach pixel
Passes
→ Semantic image segmentation is a beige task to classify
in an image from a predefined
So, Semantic segmentation is the
at of
is the classification of pixel level
+ For example, in an image with multifule objects,
know which pixel belongs to which object
we want to
Neelgagan
No.
Date
→ Our goal is to take an image of size WXHX3 and generate
a wxH matrix containing the predicted class ID's corresponding
to all the pixels.
+ In semantic segmentation, duf barning and model takes image
as input and outputs segmentation mop of the individuel objects.
detected in the image
iffimag
Encode Deodes Architecture.
DL Model
Predicted off
Segmentation mat
• The encode is mostly a CNN, which extracts meaningful
feature map from an input image, also it increases the no. of
channels in each step and downsample the image, i...
height and width
.
Med the
The decodes upsamples the feature maps and decrease
the no of channels and outputs a segmentation map, which is
the region of pixel.
#PSP Net
(Pyramid Sune Parsing Network)
semantic
No.
Date
+ PSP Net is a fully CNN for image assification segmentation-
It consists of a pyramid pooling accent module to exploit the
local and global context information.
→ PSP Net performed the segmentation on PASCAL VOC 2012 and
cityscapes benchmark datasets
+ First a feature
map is extracted from the last so convolutional
layer of convolution neural network that is fed to the pyramid
pooling module in order to harwest different pooling
pyramide. Finally, convolution layer is applied to obtain
the pixel-wise prediction
PSPNet Architecture
(A)
(B)
Input Image / CNN] [Feater Map
(c) Pyramid
Pooling
Modul
a
d Finals
Prediction
Neelgagan
Neelgagan
CNN
Con
- Sample & comy
☐
No.
Date
Given an
first use CNN to get the featury
input image (9), we
maf of the last convolutional layes (6), then a pyramid
module is applied to harvest different sub-rege
representations, followed by upsampling and concatenation
final détachers feature representation,
layers to form
parsing
which carries both local and global context information in (C).
Finally, the representation is fed
fed into convolution layer
to get the final per-pixel predictions.
tion:
# U-Net Architecture for biomedical image segmentation
+ U Net is a fully convolution neural network for image semantic
segmentation. It is used for biomedical semantic segmentation.
tation on ISBI cell
→ U-Net is proposed to perform the segmentation
tracking challenge
as
Phc - U373 dataset
→ It comprised of a contracting path that follow the same fashion.
typical architecture of a convolutional network and an
expansive path that is used for upsampling of the feature map
60689
Neelgagan
black
No.
Date
x-yoize
U-Net Architetu (example for sex32 pixels in the lowest resolutis)
fach blast box corresponds to a multi-channel featurs map.
The no of channels is denoted on top of the box. The
is prounded at the lower left edge of the box. White bones represen
copied feature maps. The arrows denote the different operations.
*PAN (Pyramid Attention Network)
T Pyramid Attention Network is proposed for semantic segmentation
It consists of feature Syramid Attention module and Global
Attention upsample Module.
T
Segmentation experimento using PAN are performed on PASCAL VOC
2012 and cityscapes benchmark datasets
→ feature Pyramid Attention module to perform spatial pyramid
attention structure on high-level output and combine global
pooling to hearn a better feature representation.
+ Global Attention Upsample module on each decode lazer to
provide global context as a guidance of low-level features to
sebit category localization details...
No.
Date
blue line
ill image
Conv-1
-2
Res-3
Res-7
Res-s
upsampl
[GAUSE GAUL GAUL [FRA]
De
red lines
We use ResNet - 101 to extract dence features. Then we perform
FPA and GAU to extract precies pixel prediction and
localization details. The blue and red lines represent the
downsample and upsample operators respectively,
(a) Spatial Lyrend footing
Res Net
Cow
32-35 Globa
Pouting Low/I
Row 1x14 seps
com Concat ResNe
Conv
Conv sxi
(a) Spatial Pyramid Pooling
5x512)
(4×4)
[Conv 3x3 con v3³×3
Featur fyra
amid Attention
Neelgagan
No.
Date
(a) Spatial Pyramid Pooling Structure. (b) features Pyramid Attention
modul, `4x4, 8x8, 16X16, 32x32' means the resolution
The dotted box means the global fooling branch.
feature map.
The upward arrow and downward arrow represent the
downsample and upsample operators respectively. Note that
a convolution layers are followed by batch normalization
PAN GAU Modul
Low-led Features
High-level
Featu
Cony 3x3
T
Conv ki
Global Pooling
(CXIXI)
#Multi-Task Contextual Network (MTC Net)
An Artificial Intelligens - based
for hospitalized patients.
No.
Date
system
he
assess nutrient intake
→ Multi-Task Contextual Network for food semantic segmentation
to assess nutrient intake for hospitalized patients.
Segmentation experiments using MTC Net are performed on Nutrient
Intake Assessment Database (NAD)
This
system consists of a novel multi-task contextual network
for food item - segmentation, classification with few-shot learning-
based algorithms built by limited training samples for
food recognition and 3D food surface extraction.
+ The proposed system provides the fully automatic salution to
assess nutrient intake for hospitalised patients because to maintain
good nutritional status is of vital importancy for both patients
and social medical systems!
Neelgayan
Global Attention Upsample module Structure
Neelgagan
No.
Date
-
-
YOLO FAMILY
# Introduction to Object Detection.
Image Classification:
+ The goal is to answer the question of what is present in the image.
→ loually only one object is present in the image -binary and multi
Jan Classification.
Then we also we have multi- label classification.
Object Detection:
• Object detection not only tells what's in the image but where in the
image is the object.
Localise each objat
Classify each of the localized object.
Challenge
-Crowded or Cluttered Sumario.
•
9
e
Intra-Ulass Variance
Class Imbalance
Deformation.
Neelgagan
# History of Object Detection
2001
VJDet.
No.
Date
Houpet. 2006
2008
DPM. + bounding box
Two
stag
RCNN
2017
foot RCNN
2015
SPPNet
2012
regression
One Stage
2016
YOLD
SSD
Multi-referen
detection.
(Anchon
boxes
faste Ren
Neelgagan
Pyramid
2017
Netwashe
featuri
fusion
2017
2019
Retino Net
Efficient Det
+ bi-direction
FPN
+ compound
scaling
No.
Date
# Single Stage Object Detectors
Image
Feature
Extractor
Featu
Map
Detechon Head
Box
Regression
Classificati
(9)
No.
Date
• Novel techniques like multi-scale training allowed the network
to predict at varying input sices.
At 416, x 416 impul resolution, YOLDR achieved 76.8 MAP
on Voc 2007 dataset and 67 FPS on Titanx GPU
#VOLOVS
Introdund ~ new network architecture called Darknet-53
YOLO VS achieved 27.2 MAP runs at 45 FPS on the Pitam & GPU.
At 320x320 with Ms Coco datas
achieved 22.27AP
#YOLO 1:
• YOLOVI was the first single-stage object detector.
• Asingle meural network predicts class probabilities and bounding
.
box coordinates
YOLO achieved 63.4 MAP
The generalizability of YOLO was tested on artwork and natural
images from the internet
#YOLO VL
Two state-of-the-art YOLU :
Neelgagan
VOLOVE and PASCAL VOC
YOL 09000
4 on 2000 classes
Coco & Imant
Visual
datant
YOLOVY
• YOLOVY performs better than Efficient Det and VOLD VS
• Achicus state of the art results: 43.5; MAP. (65-74 MAP 50)
on the MS Coco dataset at a real-time speed of nos FPS on the
Tedla Loo GPU
• Leveraged Bag- of food Freebies and Eapof- Special
helps improve the
training of the model
and does hot impart
the
inferenc opeed
65
helps improve the
accuracy of the object
detectax but at the cost of
inference opeed.
Neelgagan
No.
Date
No.
Date
#YOLO V
→ In 2020, Glenn Jocher, the founder and CEO of ultralytics,
1-source implementation of VOLOVS.
released its open.
→ YO 20 V5 offers a family of Object detection architectures like
YOLOVSH, YOLO V50, YOLO vím, etc. pre-trained on the
MS COCO dataset.
» Yαovs is natively implemented in PyTorch, eliminating
the Darknet frameworksⓇ limitations.
Code:
import torch
madil: torch. hub. hood ("ultralytics/ydors", "yalovs')
# or yolovim, yolo vse
img = "https: // ultralytics, com /images/zidan.jpg
# as file, Parts, PIL, Opencv
results model Cime)
results. printe
Neelgayan
#or, shows us save 1)
features for developers:
On Custom Dataset.
• training
Multi-GPU training
Exporting
the trained YOLO VS model on TensorRT, CoreML,
ONNX, and TFlite
• Pruning
the YOLO VS architecture
Deployment with TensoART
• ios application called iDetection, which offers four variants of
YOLO VS, the model runs detection at close to 30 FPS on iPhone 13.
Pro
Mosaic Data Augmentation:
•
9
Network ses more contextual information in one image.
Model learn's to identify objects at a smallik scale than usual
Helps in batch- normalization.
turns 4 training images intol image in specific ratio.
#PP-YOLO
-YOLO
Paddle Paddle
•Researchers of Baidu published PP-YOLO
Neelgagan
No.
Date
1. This is the 3rd frames are in which YOLO was implemented
.
On a Volta 100 GPU, PP-YOLO achieves 45.2% maland
inferemu speed of 72.1 FPS outperforming Efficient Det, RetinoNed and
YOLD
Paddl Detection
•An end-to-end detection development hit
.
It is based on a Paddle Paddle deviloped by Baidu
.
Provides loads of detection and segmentation - architectures,
backbones, necks, data augmentation strategies.
Selection of Tricks and Techniques
•
•
.
Large batch size
Exponential Moving Average
Drop Block Regularization.
IOU LON
Iou Away
•Matrix Non-May Suppression
Better Pretrained Model.
Note: PP.YOLO outperform YOLO VY, Efficient Det, RetinaNet
as well as accuracy.
in terms
Nalgayan
of speed
.
#Scaled-YOLOVY
•
No.
Date
In 2021, Chien-Yao Wang et al. published a paper titles
Scaled - VOLO v4: Scaling. Grass stage Partial Network at the
CVPR conference
Scald YOLOVY uses
Scaled-YOLOVY
large
optimal network ocaling
achieves 55.5% AP.at16 FPS and Tiny
achieves 22.0% Apat 443 FPS.
#YOLOX
•
•
In 2021, Zheng he et al. published YOLOX: Exceeding YOLD
series in 2021
Implemented in the PyTorch framework
An anchor - free detector
Jot Play on
Streaming berception Challenge
YOLOX - Nono with only 0.91M parameters, achieved 25-3%
AP on the MS Coco
YOLOX-Darknet 53
YOLO V3 with Darknet 53 backbone is selected as the baseline
Series of improvements.
were made to the base model
• Some training strategics are modified compared to the origins
YOLO VS implementation
1
Exponential - Moving Aureage weights update-
+ Cosine learning rate schedule
Neelgagan
No.
Date
No.
Date
+BCE loss for classification and objectness branch.
+10U Loss for regression brands.
•Strong Date Augmentation
Mosaic and Mixup augmentation helped improved the decoupled
head by 2·4% MAP.
Anchos-Free Detection
Helps reduce the number of predictions for each grid sell from
three to one directly predicts four values. Both model parameters
and GFLOPs of detectes are reduced making the detector faster
by 4FPS and may accurate by 0.9% MAP over previous
improvement.
Multi Positives
The unter 3x3 aree is assigned as positivities to avoid extrem
imbalance of the fave sampling. This approach further.
improves the detector by 2.1%. MAP.
# Data Loading in Pytorch.
loading and converting data into formats that are ready for
training com often end up being one of the areas in data sucnu
that sucks up for too much dous time.
The PyTorch has developed standard conventions of interacting
with data that make it fairly consistent to work with
whether you're working with images, text, or audio.
•Dataset and Data leader in lytorch
The two main ceration convention of interacting
are datasets and data loader
with dala
→ Adataset is a python class that allows us to get at the data
we're supplying
A data leader
us
to the neural network
what
feeds
data
from the datact into the
network. (This can be incompass information such as
How
many worker prousses are feeding data into the
network? or How many images.
Dataset class in PyTorch
200
we
passing
inatony?)
Every dataset, no matter whither it includes images, audio,
text, 3D landscapes, stack market information, or whaters
can interact with PyTorch if it satisfies this abstract Python
Class
I
Neelyapen
Neelgagan
from
torch. utile. dato import Dataset.
Case Dataert (object).
def_getitem__(self, inder):
rais, Not Implemented Erres
No.
Date
# face Detection Tips:
1.
2
raiss Not Implemented Error.
def. -- len - (self):
Dataset class functions in PyTorch
--getitem__
This function loads and returns a sample from the
dataset at given index, Based on the index, it identifics.
the image location on disk, convert that to a tensor, retrieves
the corresponding label from data & in, calls the transform
functions on them (if applicable) and returns the tensor imags
and corresponding lobel in a tuple.
The den function returns the no. of samples in the dataset.
Nealgagan
Fau Detectors
Open CV'. Haar Cascades
'Open CV', SSD fau Detector
3. Dlible Hoh+ Linear SVM implementation.
4. Dlib'e CNN fav detector
• Open CV's Haar Cascades.
Pros:
Pros and Con
No.
Date
• Very fast, capable of running in super real-time
• Low computational requirements - com
- lasily be run on
embedded
resource - Constrained devices.
• Small model sice (just over LOOKB; for reference most dup
neural networks will be anywhere b/w 20 hooMB)
Cons
• Highly prone to fabe
e
tuy detections
Typically requires manud tuning to the detect Multi scale
function
Not any where near as accurate as its Hon + linear sum and
deep learning -based face detection counterparts.
Neelgagan
No.
Date
No.
.
It Opencube Deep learning face detector.
Pros
.
•
.
Accurate face detector
Utilizes modern dep learning algorithms
No paramites tuning required
Can run in real-time on modum laptops and desktops
Model is reasonably sized (
Relies on open (Vo cv2. dnn module.
Can be made
ou 10 MB)
Just ones
faster on embedded devices by using open VINO
and the Movidius NCS
Cons
Much accurate than Haar Cascades and Hout Linear SVM, but
not as accurate as dlib's CNN MMOD face detectar
May have unconscious biases in the training set - may not
detect darker - shinned people as accurately as lighter skinned
people
III Dlib's HOG + Linear SVM faudtector.
•
.
bros
More accurate than Haar Cascades
More stable detection than Moar Cascades (ie., fewer parameters
to tume)
Noelgagun
B
Date
•Expertly implemented by dlib creator and maintaines, Davis
king.
- Extremely well documented (both in implementation and
Z
Cons
papers)
• Only
works on frontal views of the face due to matur
descriptor (ive., profile will fail)
of HOG
•Requires an additional library (dlib) be installed - not necessarily
a problem, just be aware of it.
Not as accurate as
deep learning -based face detectors
• for the accuracy, it's actually quite computationally expensive
due to image pyramid construction, sliding windows,
and computing HOG features at every stop of the window
II Dlib'o CNN fauditedor.
Pro
• Incredibly accurate face detector
Small model size (under 1 MB)
• Expertly implemented and documented.
Cons
•Requires
am
additional library (dlib) be installed
Code is more verbose - end-usq. must take care to conuut and
trim bounding box coordinates if using opencv
Neelgagan
No
Date
Cannot run in real-time without GPU acceleration.
'Not out of the -box compatible for acceleration via Open VINO,
Movidin NCS, NVIDIA Jetson Nano, or Google catal
• Suggested One: OpenCV's DNN face detector
- It achieves a nice balance of speed and accuracy
7 Ao a deep learning - based director, it's more accurate than
it's Haar and Hout Lineas SVM counterparts.
" It's fast enough to run real-time on Clvo.
+ It can be furthes accelerated using USB devices such as Movidius
NCS
required - support for
+ No additional libraries/ packages are
face detectos is baked into OpenCV via the cv2. dnn module
the
#Face Recognition
Recognizing the faces.
• Face Recognition Steps •
Step #1:
No.
Date
dcept impurt image, apply face detection, and extract fau Roj
Step #2
Take face ROI and apply face Recognition.
History of Face Recognition.
- conceived in early 1970s but wasn't until 2015 that we
Obtained reasonable accuracy
-
- Prior to 1970s thought of a science fiction.
- Goldstein (19700) 21 subjective facial land mask there wer
manually placed ( i.e., not automatic)
-
-
Sirovich and Kirby (1987): Eigen pictures (linear algebra band
approach to face recognition)
Cuskand Pentland (1991): Eigenfaces (seminal works
Ahonen et al, (wow) : LBPO for face recognition (feature-based
Neelgagan
approach).
-
No.
Date
• Deep Learning -based methodo (2015-) : fauNet, OpenFace
• Eigenfaus
-
-
1
siamese networks, etc.
PCA to construct low dimensional representation of faces.
Faus must be aligned and have same width and height.
Convert to grayscale and flatten into ID list of pixel values.
Computed Eigen vale decomposition.
Keep eigen vecture with largest eigenvalues.
A face can be represented as a lineas combination of
Eigenfaces
The R-NN to recognize faus (or other standard ML algorithme
· LB1o for face Recognition
7
features extraction-based approach
Divide imput face into NEM cells (typically 7x7048x8)
- Compute LBPs for each w
- Concatenate histograms
-
Use k-nn or other ML algorithm to recognize faces.
This method tends to be a bit more accurate and less
susceptible to noise than Gigenfaus
• Deep Learning for Face Recognition:
No
Date
- High- accusary for recognition really wasn't possible until
DL-based methodo
-fauNet and Open Face
- Siamese networks, triplet images, and triplet loss
Easily beats other face recognition methods - but is
more computationally expensive".
Ely
APCA Algorithm
significant
1. Compute the mean μ; of each column in matrix, giving
us the average pixel sition intensity value for every (xy)-
Coordinates in the
image
dataset
2. Subtract the μi from each column (; - this is called mean
centering the data and is a required step when performing PCA.
3. Now that the matrix μ has been mean centered, compute the
covariance matrix.
4. Perform an eigenvalue decomposition on the covarianc
matrix to get the ligenvalues di and eigenvectors X;.
5. Sort x; by xil; largest to smallest.
6. Take the top N eigenvectors with the largest corresponding
eigen valu magnitud.
7. Transform the input data by projecting (ie., taking the dot
product) it onto the space vecated by the top N eigen vectors
- these eigenvectors are called our eigen fous !
Neelgagan
No.
Date
No.
Date
.
LBP
weighing scheme
con
weighed
• LBP histograms for the white all couch as the eyes)
4x more than the other cells. This simply
• that we take the LBP histograms from the white
all regions and multiply themby & (takity in the account any
scaling/ normalization of the histograms).
4
meame
Light Gray ullo ( mouth and ears) contribute 2x more.
gray cells (innese check and forehead) only
Dark
contribute Ix.
• Finally, the black cells, such as the nose and outer check, are
totally disregarded and weighed ox
Face Recognition Using FauNet Keras"
•Google's dup convolutional network - FuciNet profond in 2015
by Google Researchers overcomes the hindrance fimplementing
face verification and recognition efficiently at scale and
achieved the state of the art results.
The authons introduced us to the concept of harmonic embeddings,
and a harmonic triplet loss, which describes different versions
of face embeddings (produced by different networks) that are
Compatible with each other and allow for direct comparison
b/w each other.
FauNet is the backbone of many opensource systems such as
FauNet using Tensorflow, Keras, FaciNet, Defface, Openface
- The complete face recognition system com be divided into three
categories:
Face Detection
2. Feature Extraction
3. Feature Matching
- Fau Detection:
The face detection method is used to find the faces present in the
image, extract the fans, and display it (or create a compressed
file ho
The use it further for feature extraction).
Neelgagan
No.
Date
No.
Methods used in faue Detection:
1. Haar Cascade Face Detection:
This method has a simple architecture that works nearly
real-time on CPU. Also, it can detect images at different
scales. But the major drawback is that it
as well as it don't wash on non-
2. Dlib (How Face Detection
gives false results
-frontal images.
frontal
It is the fastest method on CPU which can work on
and slightly no-frontal images. But it is incapable of detecting
small image and handling occlusions. Also, it often excludes
som parts of the chin and forehead while detection.
3. Dlib (CNN) fau Detection!
It warks very fast
on GPU and is capable to work for various
face orientations in images. It can also handle sxclusions.
But the major disadvantage is that it is trained on a
min face size of 80 × 80 so it can't detect small faces in
images.
4. MTCNN
It is also
very
slow onth CPU.
This method gives the most accurate results out of all the four
methods. It works for fales having various orientations in
images and can detect across various scales. It can
wen handle oulusions. It doesn't hold
Date
as such but is comparatively slower than Hoh and Haar Cascad
any major drawback
method
Note: Hence, we will be using the MTCNN face Detection methot.
Neelynpun
Neelgagan
No.
Date
No.
Date
#Object Tracking
- Tracking the movement of an object has many afflications,
to implementing object
from tracking robots in a warehouse
tracking systems in drones.
- The basic of Object tracking rely on object detection, but the object,
in this case, is vicived from different angle
M
Object Tracking?
• What is Object
-
-It is a computer vision application where a program detects
objects and then tracks their movements in space er
different camere angles.
acrow
Object tracking com identify and follow multiple objects in
for example, a football recording studio.
am
image
could follow where a ball is inphoto.
Object tracking is a significant computer vision technology
popular in augmented reality for estimating orpredicting
the positions and other applicable information of moving
objects in real time.
Object detection algorithme identify objects in an imge, or video
and their location in the media. This cam be an algorithm
on its own, or used to enable abject tracking. Object tracking
algorithms, on the other hand, follow objeto over
Neelguy
avideo.
frames in
• How Object tracking works:
-
Ilp video file,
Object Detection and
Classification
Object Tracking
→ Input: The first step is to give i/p such as video or a real-time
Yash feed from a camera and preprous each frame
->
->
using open cv. Pre-processing
is essential so the
model has consistent data such with which to work.
Object Detection: Next, we choosean
Object detection
algorithm that classifies and detects the object
by creating abbox around it.
Labelling: Next, the object tracking algorithm, assigns
a unique identification label for each object that
has been identified. For example, this would be all
of the cars in a video feed of a racing track.
Tracking: The last step is keeping track
of the detected
object moving through different frames while
its relevant path information.
Storing
Neelgagan
.
No.
Date
No.
Date
Types of Object Tracking.
1- Single Object Tracking (SOP):
opecific target
Single Object Tracking aims to track one
object throughout the video. The object tracking algorithm
starts with defining the bbox of the target object in the first
frame and then locates the same object in the rest of the frames.
2. Multiple Object tracking (MOT):
more than one
Multiple Object Tracking (MOT), it is a more complex type of
tracking since it involves detecting and tracking
object in a vidu. Initially, the object tracking algorithm
determines the number of objects in each frame, identifies them,
draws a bbox around them, assigns Object a unique
coordinats, and keeps track of each object's movement over
back-to-back frames until they leave the frame
Open Soure tools for object tracking:
1. Object tracking with Opencv
lach
2. Fast online object traching with Siam Mask
3. Zero Shot object tracking implemented with the Roboflow
Inference Art, Deepsort, and Open AI CLIP
4. Non fair is a python library for adding real-time multi-object
tracking to any
detector.
·S. Object tracking using Roboflow Inference API and zeroshot CLIP
Deepsort.
6. YOLOVY, DeepSORT and Tensorflow implementation of object Traching
7. Towards grand unification of object tracking.
Use Cases
1. Autonomous Driving Obstacle detection in self-driving cars,
fedestrians detection on roads and sidewalks, collisions
avoidance and vehicle speed detection, traffic monitoring.
and route estimation, etc.
2. Retail Applications: Person detection and tracking, cashier - less checkout-
systems, tracking items in and out of a shapping cart, tracking
people leaving or entering the buildings, etc.
3. Surveillany Cameras: browd monitoring, tracking mask usage,
2
tracking people with suspicious activities on items, itc
4. Sports Analytics Tracking the trajectory of a basketball, traching
football to count the number of goals, tracking players to
understand their movements and preventing injuries, etc.
Neelgugan
Neelgagan
No.
Date
No
Date
#Key Point Detection:
head).
detection is
•Keypoint detection is a computer vision task that aims to identify
the location of an object often a person - and key points within
Keypoint
the identified areas Live., legs, arms,
at the heart of many cutting-edge technologies, enabling applications.
from facial recognition in omartphones, assisting in object tracking
for autonomous vehicles, or aiding in medical imge analysis.
• What is Keypoint Detection?
-Keypoint detection invalues identifying specific, distinct points or locations
within an image or frame in a video. These distinctive points, often
referred
to as
serve as landmarks or reference markers.
These markers combe used by machines to analyze and interpreet.
"keypoints"
the visual content of an image.
building
am
For instant, consider a scenario where we are
exercise application that lits people practice new yoga poses.
We could use key point to identify various parts of the
person using the application their hips, legs, knees, elbows, arms -
and apply logic to identify when the user success fully poses
according to a visual prompt of ayoga posse
There are few sub types of keypoint detection, which include:
1. Human Pose Detection
Eoti maha
"Identify key points associated with
people
2. Hand Rose Estimation: Identify key pointo associated with
3. Facial Key Points:
4. Animal Key
human hands.
Points: Identify key points on human faus.
Pointo: Identify key points on animals (ie, catafica,
What makes a
Point
a
" Key Point"?
Key points are typically defined by certain characteristics that set
mem apart from the surrounding pixels. These characteristi
includ
1. Uniqueness: Key points should be unique and easily distingui
-shable from other points in the image. They stand out due to
specific visual attributes, such as color, intensity or texture.
2. Invariance: Key points should exhibit a degree of invarianc
to common image transformations, such as rotation, scaling,
in lighting conditions. In other words, the same
hey point should be detectable in different versions of the
object as sune.
and
changes
Danme
Neelgagan
instamus
No.
Date
13. Planes of the same object or sums. This
3. Repeatability: Key points should be reliably detectable across
different
repelatability is essential for various applications including
object recognition and tracking.
• How to Detect Key Points
we need
The keypoint detection process is similar to that of any other
computer vision proum, but with one big difference:
to annotate images with key points that we want to
identify
The prouse of heypoint detection typically consists of the
"following key steps:
1. Data Preparation: Collect and annotate a dataset of images
. with hey points.
2. Model Selection and Training: Choose a deep learning architecture
suitable for keypoint detection and train it on the annotated
dataset. The model should learn to predict key points
board on image imput.
a
3. Model Evaluation: Evaluate the model's performance using
separate validation dataset: Metrics like Mean Average
Prebion (mAP) or Euclidean distance error combe word
to assess keypoint detection accuracy
No....
Date
4. Detection: Use the led trained mode for keypoint
detection on new, unseen images. Provide the image as
input to the model, and it will predict the key points.
Common Techniques and Algorithms for keypoint
Detection
- Keypoint detection has evolved significantly with the advent
advent of deep learning techniques. Deep learning
models have demonstrated remarkable capabilities in
various computer vision tasks, including keypoint detection
- Some deep learning techniques:
1.YOLO
- while 2010 is primarily used for object detection, it can be
adapted for heypoint detection tasks. To achieve this, we can
add extra output layers in the network that predict keypoint
coordinates. Each keypoint corresponds to a set of output
channels in these layers.
For instance, in Pose TED, YOLO-14 is employed for human
detection, resulting in the localization of individuals using
bounding boxes. Subsequently, the spatial Transformes. Net work
(STN) is applied to extract regions of interest by cropping
The original sing based on the predicted 6boxes.
Neelgagan
Neelgagan
No.
Date
The backbone network then processes these regions to generate
comprehensive feature representations. The keypoint detection
prouess, relative to the corresponding bounding boxes, is
sans facilitated by a Transformer encody-duodes with
positional encoding..
Ultimately, a prediction -based feed-forward Network (FFN)
is utilized to predict key points and represent them as
vectors corresponding to various body parts.
Note: In addition, the ultralytics YOLOV8 architecture has
support for keypoint detection.
2. OpenPose
-
uses a convolutional
Openlose, a computer vision framework,
neural network backbone to simultaneously detect multiple
body parts, including joints, banda, fut and the face,
input images or video frames
in
By generating confidence maps and part affinity fields CPAFO),
OpenPose captures the likelihood of body part presence and
spatial relationships b/w parts. Post-processing techniques
are then applied to identify key points and their connections,
enabling the estimation and visualization of human posis
The resulting keypoint coordinates find applications in gestury
No.
Date
recognition, action recognition, fitness tracking, and other fields,
making open Pose a valuable tool for human poes estimation.
3. Keypoint RCNN:
-
Keypoint RCNN is a keypoint detection framework that extends
the faster RCNN object detection model. It first employs a
Region bropasal Network (RPN) to generate region proposals
and them fine-tunes these proposals using a CNN - board
Object detection head.
- In addition to object bounding boxes, Keypoint- RCNN
simultaneously predicts Luzpoint locations within
each proposals, leveraging a keypoint head.
-
The model is trained using annotated datasets that include
both object labels and kypoint amnotations and it employs
a multi-task loss function to optimize both object detection
and heypoint estimation. This enables Keypoint - RCNN to
accurately locate and predict key points for objects within
images, making it valuable for applications such as human pose
stimation and object manipulation stacks.
4.Center Net
Center Net is an
Object detection framework that works by
identifying objecta centers and their associated keypoint
locations. It adopts a single-stage approach by predicting object
Neelgagan
No.
Date
center points as heat maps and their size as regression values.
Keypoint locations are simultaneously predicted within
each object's bounding box.
- By using a keypoint head, Center Net can detect and estimate
Jaye key points in as unified model. This architecture
simplifies the object detection process, reduus computation,
and has shown strong performance in tasks like human
pose estimation and object detection, making it a versatile
solution for various applications
Neelgagan